{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generated the data from solver for taining the data driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples ko shape:(1000, 11, 28, 28)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'num_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 131\u001b[0m\n\u001b[0;32m    125\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Save the training and testing data\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# print(\"Saving training data to u_train.npy...\")\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriven_data_28x28_1k_11timestep.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), np\u001b[38;5;241m.\u001b[39marray(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'num_samples'"
     ]
    }
   ],
   "source": [
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.random as random\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.fft as jfft\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "from numpy import sqrt\n",
    "from numpy import round\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import contour\n",
    "from jax.numpy.fft import fft2, ifft2\n",
    "from jax.numpy.fft import fftn, ifftn\n",
    "from numpy import real\n",
    "from jax.example_libraries.stax import serial, Gelu\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "import os \n",
    "# @partial(jit, static_argnums=(0,))\n",
    "def allen_cahn_equation(uk, pp2, qq2, dt, eps, Nt):\n",
    "\n",
    "    cahn = eps**2\n",
    "    samples_timesteps = []\n",
    "\n",
    "    for iter in range(0, Nt+1):\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + dt * (2 + cahn * (pp2 + qq2))\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk)) \n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        uk = jnp.real(uk)\n",
    "        if iter%4000 == 0:\n",
    "            samples_timesteps.append(uk)\n",
    "\n",
    "        # Return the real part\n",
    "    return jnp.array(samples_timesteps)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# difinning the no of grid points in x, y and z\n",
    "Nx = 28 # number of grid points in x be positive even integer number\n",
    "Ny = 28 # number of grid points in y be positive even integer number\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters of the Allen-Cahn equation in 2d\n",
    "Lx = 2.0 * jnp.pi #length of the domain in x\n",
    "Ly = 2.0 * jnp.pi #length of the domain in y\n",
    "hx = Lx / Nx #spatial step size in coordinate x\n",
    "hy = Ly / Ny #spatial step size in coordinate y\n",
    "dt = 0.0001 #time step size\n",
    "T = 4 #final time\n",
    "Nt = int(jnp.round(T/dt)) #number of time steps\n",
    "ns = Nt / 10 #number of snapshots\n",
    "\n",
    "# Define the grid points in x and y direction\n",
    "def x_gridpoint(Nx, Lx, hx):\n",
    "    x = jnp.linspace(-0.5*Lx+hx,0.5*Lx,Nx)\n",
    "    return x\n",
    "x = x_gridpoint(Nx, Lx, hx) #number of grid points in x direction and step size and limitation on x  axis\n",
    "def y_gridpoint(Ny, Ly, hy):\n",
    "    y = jnp.linspace(-0.5*Ly+hy,0.5*Ly,Ny)\n",
    "    return y\n",
    "y = y_gridpoint(Ny, Ly, hy) #number of grid points in y direction and step size and limitation on y  axis \n",
    "\n",
    "# creating meshgrid in x and y direction\n",
    "xx,yy = jnp.meshgrid(x,y) #creating meshgrid in x and y direction \n",
    "\n",
    "epsillon = 0.5 #small parameter # interface thickness in the Allen-Cahn equation \n",
    "cahn = epsillon**2 #cahn number  \n",
    "\n",
    "# theta = jnp.arctan2(yy, xx)\n",
    "#   # or another appropriate value\n",
    "# uk = jnp.tanh((1.7 + 1.2 * np.cos(6 * theta)) - jnp.sqrt(xx**2 + yy**2) / (jnp.sqrt(2) * epsillon))\n",
    "data = np.load('data_generation_checking/phasefield2d_data_28x28_10k.npy')\n",
    "\n",
    "# Select 1,000 random samples\n",
    "key = jax.random.PRNGKey(0)  # Random seed for reproducibility\n",
    "idx = jax.random.choice(key, data.shape[0], shape=(1000,), replace=False)  # Random 1k indices\n",
    "input_samples = data[idx]  # Shape: (1000, Nx, Ny)\n",
    "# print(f'uk ko shape:{uk.shape}')\n",
    "\n",
    "\n",
    "# defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / Lx * jnp.arange(0, Nx//2), 2 * jnp.pi / Lx * jnp.arange(-Nx//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / Ly * jnp.arange(0, Ny//2), 2 * jnp.pi / Ly * jnp.arange(-Ny//2 , 0)])\n",
    "\n",
    "\n",
    "# square of wavenumber in x and y direction\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "\n",
    "# creating meshgrid in x and y direction for square of wavenumber\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "\n",
    "\n",
    "input_samples= input_samples.reshape(-1, Nx , Ny)\n",
    "\n",
    "samples = []\n",
    "\n",
    "for uk in input_samples:\n",
    "   \n",
    "    ac_input = allen_cahn_equation(uk, pp2, qq2, dt, epsillon, Nt)\n",
    "# print(f'shape of ac_input:{ac_input.shape}')\n",
    "    samples.append(ac_input)\n",
    "samples = jnp.array(samples)\n",
    "print(f'samples ko shape:{samples.shape}')\n",
    "      \n",
    "\n",
    "# Specify the directory where y want to save the data\n",
    "save_dir = './data_driven/'\n",
    "\n",
    "    # Ensure the directory exists, create it if not\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and testing data\n",
    "# print(\"Saving training data to u_train.npy...\")\n",
    "np.save(os.path.join(save_dir, \"driven_data_28x28_1k_11timestep.npy\"), np.array(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded_samples ko shape:(1000, 11, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "loaded_samples = np.load(os.path.join(save_dir, \"driven_data_28x28_1k_11timestep.npy\"))\n",
    "print(f'loaded_samples ko shape:{loaded_samples.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy required code from model training, here we are calculating the data driven loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "from spifol_archs import FNOBlock2D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # Initialize the network based on architecture type\n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(random.PRNGKey(1234), (-1, N, N, 1))\n",
    "            \n",
    "        elif arch == 'MLP':\n",
    "            self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        elif arch == 'modified_MLP':\n",
    "            self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "            )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        if self.arch == 'FNO':\n",
    "            \n",
    "            input_FNO = uk.reshape(-1, self.N, self.N, 1)  # Reshape for FNO\n",
    "    \n",
    "            O = self.N_apply(params, input_FNO)  # Apply the FNO network \n",
    "            O = O.reshape(self.N, self.N, 1)  # Reshape output\n",
    "            return O\n",
    "        elif self.arch == 'MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)  # Directly apply the network\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "            return O\n",
    "        elif self.arch == 'modified_MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "      \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def allen_cahn_equation(self, uk):\n",
    "        \n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))\n",
    "        # print(\"Denominator shape:\", denominator.shape)\n",
    "\n",
    "        # Expand the denominator to match the shape of s_hat (28, 28, 1)\n",
    "        denominator = denominator[..., None]  # Add a third dimension to make the shape (28, 28, 1)\n",
    "        # print(\"Denominator shape after expansion:\", denominator.shape)\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        # print(\"Shape of s_hat (after fft2):\", s_hat.shape)\n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "        # print(\"Shape of v_hat (after division):\", v_hat.shape)\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        # print(\"Shape of uk (after ifft2):\", uk.shape)\n",
    "\n",
    "        uk = uk.reshape(self.N, self.N, 1)  # Reshaping to (N, N, 1)\n",
    "        # print(\"Shape of uk after reshaping:\", uk.shape)\n",
    "\n",
    "        # Return the real part\n",
    "        return jnp.real(uk)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, uk):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        \n",
    "        u_nn = self.operator_net(params, uk) # predicted or next value of the initial condition\n",
    "        u_nn = u_nn.reshape(self.N, self.N, 1)     \n",
    "        u_ac = self.allen_cahn_equation(uk)\n",
    "        datadriven_loss = jnp.mean((u_ac - u_nn) ** 2)\n",
    "        return datadriven_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch):\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0))(params, batch)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, uk):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, uk)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, dataset, data_test, nIter=10000):\n",
    "        data = iter(dataset)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch = next(data)\n",
    "            batch = jnp.array(batch)\n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss = self.loss_batches(params, batch)\n",
    "                loss_test = self.loss_batches(params, data_test)\n",
    "                self.train_losses.append(loss)\n",
    "                self.test_losses.append(loss_test)\n",
    "                pbar.set_postfix({'train Loss': loss, 'test loss': loss_test})\n",
    "\n",
    "\n",
    "    def pred(self, data_test):\n",
    "        uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            uk = self.allen_cahn_equation(item)\n",
    "            # cahn = eps**2\n",
    "            # uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # # Compute denominator in Fourier space\n",
    "            # denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # # Perform FFT calculations\n",
    "            # s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            # v_hat = s_hat / denominator  \n",
    "            # uk_ac = jfft.ifft2(v_hat)  \n",
    "            uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(uk_solver, (uk_solver.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_solver, uk_nnetwork\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.05 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.0001 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "# print(f'pp2 shape:{pp2.shape}')\n",
    "# print(f'qq2 shape:{qq2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "# Define FNO layers\n",
    "fno_layers = [\n",
    "   Dense(64),\n",
    "   Permute(\"ijkl->iljk\"),\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,  # activation can be changed here\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,\n",
    "   FNOBlock2D(15),\n",
    "   Permute(\"ijkl->iklj\"),\n",
    "   Dense(128),\n",
    "   Gelu,\n",
    "   Dense(1),\n",
    "]\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 15000\n",
    "\n",
    "data = np.load('data_generation_checking/phasefield2d_data_28x28_10k.npy')\n",
    "# normalized_data, min_val, range_val = normalize(data) \n",
    "# Generate the data trainig samples\n",
    "dataset = DataGenerator(data[:9800], batch_size=20)\n",
    "data_test = data[9899:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "NN_model.train(dataset, data_test, nIter = epochs)\n",
    "r2, mse, l2_rel, u_solver, u_pred = NN_model.pred(data_test)\n",
    "print(f'r2:{r2},mse : {mse}, l2_rel : {l2_rel}')\n",
    "\n",
    "NN_model.plot_losses(f'plots/training_log_iter_{epochs}.png')\n",
    "\n",
    "\n",
    "import pickle\n",
    "# saving the parameter\n",
    "def save_model(model, filename):\n",
    "    # Save model parameters, architecture and optimizer state\n",
    "    save_dict = {\n",
    "        'arch': model.arch,\n",
    "        'N': model.N,\n",
    "        'lr': model.lr,\n",
    "        'eps': model.eps,\n",
    "        'pp2': model.pp2,\n",
    "        'qq2': model.qq2,\n",
    "        'dt': model.dt,\n",
    "        'L': model.L,\n",
    "        'h': model.h,\n",
    "        'x': model.x,\n",
    "        'y': model.y,\n",
    "        'params': jax.device_get(model.get_params(model.opt_state)),\n",
    "        'train_losses': model.train_losses,\n",
    "        'test_losses': model.test_losses,\n",
    "        'opt_state': jax.device_get(model.opt_state),  # Save optimizer state too\n",
    "    }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "\n",
    "save_model(NN_model, f'models/savemodel_{epochs}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
