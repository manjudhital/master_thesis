{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first checking the implicit method is correct or not in the numerical solution of the Allen-Cahn eqaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.random as random\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.fft as jfft\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "from numpy import sqrt\n",
    "from numpy import round\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import contour\n",
    "from jax.numpy.fft import fft2, ifft2\n",
    "from jax.numpy.fft import fftn, ifftn\n",
    "from numpy import real\n",
    "from jax.example_libraries.stax import serial, Gelu\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "# Defining the number of grid points in x and y\n",
    "Nx = 128  # number of grid points in x (positive even integer)\n",
    "Ny = 128  # number of grid points in y (positive even integer)\n",
    "\n",
    "# Define the parameters of the Allen-Cahn equation in 2D\n",
    "Lx = 2.0 * jnp.pi  # length of the domain in x\n",
    "Ly = 2.0 * jnp.pi  # length of the domain in y\n",
    "hx = Lx / Nx  # spatial step size in x\n",
    "hy = Ly / Ny  # spatial step size in y\n",
    "dt = 0.0001  # time step size\n",
    "T = 4  # final time\n",
    "Nt = int(jnp.round(T / dt))  # number of time steps\n",
    "ns = Nt / 10  # number of snapshots\n",
    "\n",
    "# Define the grid points in x and y direction\n",
    "def x_gridpoint(Nx, Lx, hx):\n",
    "    x = jnp.linspace(-0.5 * Lx + hx, 0.5 * Lx, Nx)\n",
    "    return x\n",
    "\n",
    "x = x_gridpoint(Nx, Lx, hx)\n",
    "\n",
    "def y_gridpoint(Ny, Ly, hy):\n",
    "    y = jnp.linspace(-0.5 * Ly + hy, 0.5 * Ly, Ny)\n",
    "    return y\n",
    "\n",
    "y = y_gridpoint(Ny, Ly, hy)\n",
    "\n",
    "# Create meshgrid in x and y direction\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "# Define parameters for the Allen-Cahn equation\n",
    "epsillon = 0.05  # small parameter (interface thickness)\n",
    "cahn = epsillon**2  # Cahn number\n",
    "\n",
    "# Initial condition of Allen-Cahn equation\n",
    "uk = jnp.tanh((2 - sqrt(xx*2 + yy*2)) / (sqrt(2) * epsillon))\n",
    "\n",
    "# Define the wavenumber in x and y direction in Fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / Lx * jnp.arange(0, Nx // 2), 2 * jnp.pi / Lx * jnp.arange(-Nx // 2, 0)])\n",
    "q = jnp.concatenate([2 * jnp.pi / Ly * jnp.arange(0, Ny // 2), 2 * jnp.pi / Ly * jnp.arange(-Ny // 2, 0)])\n",
    "\n",
    "# Square of wavenumber in x and y direction\n",
    "p2 = p**2\n",
    "q2 = q**2\n",
    "\n",
    "# Create meshgrid for square of wavenumber\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "\n",
    "# Plotting the initial condition of the Allen-Cahn equation\n",
    "figure1 = plt.figure()\n",
    "plt.contour(x, y, jnp.real(uk.T), [0], colors='black')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('Initial condition')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Define the nonlinear function and its derivative\n",
    "def f(u):\n",
    "    return (u**3 - 3 * u)\n",
    "\n",
    "def f_prime(u):\n",
    "    return 3 * u**2 - 3\n",
    "\n",
    "# Newton's method for implicit scheme\n",
    "def newton_method(u_hat, dt, eps, pp2, qq2):\n",
    "    def residual(u_hat_new):\n",
    "        u_new = jfft.ifft2(u_hat_new)\n",
    "        u_old = jfft.ifft2(u_hat)\n",
    "        return jfft.fft2((u_new - u_old) / dt - jnp.gradient(u_new) + (u_new*3 - u_new) / eps*2)\n",
    "\n",
    "    def jacobian(u_hat_new):\n",
    "        u_new = jfft.ifft2(u_hat_new)\n",
    "        return jfft.fft2(1 / dt - jnp.gradient(u_new) + (3 * u_new*2 - 1) / eps*2)\n",
    "\n",
    "    u_hat_new = u_hat\n",
    "    for _ in range(10):  # Newton iteration\n",
    "        res = residual(u_hat_new)\n",
    "        jac = jacobian(u_hat_new)\n",
    "        u_hat_new = u_hat_new - res / jac\n",
    "\n",
    "    return u_hat_new\n",
    "\n",
    "# Time stepping\n",
    "for iter in range(1, Nt):\n",
    "    u_hat = jfft.fft2(uk)\n",
    "    u_hat = newton_method(u_hat, dt, epsillon, pp2, qq2)\n",
    "    uk = jfft.ifft2(u_hat)\n",
    "    uk = jnp.real(uk)\n",
    "\n",
    "    if iter == 2000:\n",
    "        plt.contour(x, y, jnp.real(uk.T), [0], colors='red')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "    if iter == 4000:\n",
    "        plt.contour(x, y, jnp.real(uk.T), [0], colors='green')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "    if iter == 10000:\n",
    "        plt.contour(x, y, jnp.real(uk.T), [0], colors='blue')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "    if iter == 20000:\n",
    "        plt.contour(x, y, jnp.real(uk.T), [0], colors='red')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.title('Numerical Solutions after iteration ' + str(Nt))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft, jfft2\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "from spifol_archs import FNOBlock2D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        # Initialize the network based on architecture type\n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(random.PRNGKey(1234), (-1, N, N, 1))\n",
    "            \n",
    "        elif arch == 'MLP':\n",
    "            self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        elif arch == 'modified_MLP':\n",
    "            self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "            )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        if self.arch == 'FNO':\n",
    "            \n",
    "            input_FNO = uk.reshape(-1, self.N, self.N, 1)  # Reshape for FNO\n",
    "    \n",
    "            O = self.N_apply(params, input_FNO)  # Apply the FNO network \n",
    "            O = O.reshape(self.N, self.N, 1)  # Reshape output\n",
    "            return O\n",
    "        elif self.arch == 'MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)  # Directly apply the network\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "            return O\n",
    "        elif self.arch == 'modified_MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "      \n",
    "\n",
    "    def allen_cahn_equation(self, uk):\n",
    "         \n",
    "        \n",
    "            # Expand pp2 and qq2 to include a channel dimension\n",
    "        self.pp2 = jnp.expand_dims(self.pp2, axis=(0, -1))                                                                                 \n",
    "        self.qq2 = jnp.expand_dims(self.qq2, axis=(0, -1)) \n",
    "    \n",
    "\n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))  \n",
    "        \n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        v_hat = s_hat / denominator  \n",
    "        uk = jfft.ifft2(v_hat)  \n",
    "        uk = uk.reshape(self.N, self.N, 1)\n",
    "     \n",
    "        return jnp.real(uk) # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, uk):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        \n",
    "        u_nn = self.operator_net(params, uk) # predicted or next value of the initial condition\n",
    "        u_nn = u_nn.reshape(self.N, self.N, 1)     \n",
    "        u_ac = self.allen_cahn_equation(uk)\n",
    "        physical_loss = jnp.mean((u_ac - u_nn) ** 2)\n",
    "        return physical_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch):\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0))(params, batch)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, uk):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, uk)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, dataset, data_test, nIter=10000):\n",
    "        data = iter(dataset)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch = next(data)\n",
    "            batch = jnp.array(batch)\n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss = self.loss_batches(params, batch)\n",
    "                loss_test = self.loss_batches(params, data_test)\n",
    "                self.train_losses.append(loss)\n",
    "                self.test_losses.append(loss_test)\n",
    "                pbar.set_postfix({'train Loss': loss, 'test loss': loss_test})\n",
    "\n",
    "\n",
    "    def accuracy(self, data_test, pp2, qq2):\n",
    "        uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "\n",
    "        \n",
    "\n",
    "        pp2 = jnp.expand_dims(pp2, axis=(0, -1)) \n",
    "        qq2 = jnp.expand_dims(qq2, axis=(0, -1))  \n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            cahn = eps**2\n",
    "            uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # Compute denominator in Fourier space\n",
    "            denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # Perform FFT calculations\n",
    "            s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            v_hat = s_hat / denominator  \n",
    "            uk_ac = jfft.ifft2(v_hat)  \n",
    "            uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(uk_solver, (uk_solver.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_solver, uk_nnetwork\n",
    "\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.05 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.01 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "# Define FNO layers\n",
    "fno_layers = [\n",
    "   Dense(64),\n",
    "   Permute(\"ijkl->iljk\"),\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,  # activation can be changed here\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,\n",
    "   FNOBlock2D(15),\n",
    "   Permute(\"ijkl->iklj\"),\n",
    "   Dense(128),\n",
    "   Gelu,\n",
    "   Dense(1),\n",
    "]\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 30000\n",
    "\n",
    "data = np.load('data_generation_checking/phasefield2d_data_28x28_10k.npy')\n",
    "# normalized_data, min_val, range_val = normalize(data) \n",
    "# Generate the data trainig samples\n",
    "dataset = DataGenerator(data[:9800], batch_size=20)\n",
    "data_test = data[9800:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "NN_model.train(dataset, data_test, nIter = epochs)\n",
    "r2, mse, l2_rel, u_solver, u_pred = NN_model.accuracy(data_test, pp2, qq2)\n",
    "\n",
    "NN_model.plot_losses(f'plots/training_log_iter_{epochs}.png')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
