{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loaded the required data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded_samples ko shape:(1000, 41, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# loaded the generated data\n",
    "# loaded_input_samples = np.load(\"../data_driven/data/driven_data_28x28_1k_input_samples_20timestep_every2kiter.npy\"))\n",
    "# print(f'loaded_input_samples ko shape:{loaded_input_samples.shape}')\n",
    "\n",
    "loaded_samples = np.load(\"../data_driven/data/driven_data_28x28_1ksample_41timestep_every1kiter.npy\")\n",
    "print(f'loaded_samples ko shape:{loaded_samples.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch_input_train: (20, 40, 28, 28, 1)\n",
      "Shape of batch_label_train: (20, 40, 28, 28, 1)\n",
      "Flattened input size: 31360\n",
      "Weight shape: (31360, 64)\n",
      "Flattened input size: 2007040\n",
      "Weight shape: (2007040, 128)\n",
      "Flattened input size: 4014080\n",
      "Weight shape: (4014080, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input size inside loss_single is (40, 28, 28, 1)\n",
      "Shape of uk before reshaping: (40, 28, 28, 1)\n",
      "Shape of uk after adding batch dimension: (1, 40, 28, 28, 1)\n",
      "Input shape before permuting: (1, 40, 28, 28, 1)\n",
      "Output shape after permuting: (1, 1, 40, 28, 28)\n",
      "Shape of uk after permuting: (1, 1, 40, 28, 28)\n",
      "Before Dense - Input shape: (1, 1, 40, 28, 28)\n",
      "Dense Layer - Flattened Input shape: (1, 31360)\n",
      "Dense Layer - Weight shape: (31360, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 5-dimensional input, got 2 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 807\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m#    # Initialize and train the model\u001b[39;00m\n\u001b[0;32m    806\u001b[0m NN_model \u001b[38;5;241m=\u001b[39m SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 807\u001b[0m \u001b[43mNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# Now call loss function\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# loss = NN_model.loss_batches(params, batch_input, batch_label)\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# r2, mse, l2_rel, u_pred = NN_model.pred(test_input, test_label)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    816\u001b[0m \n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# save_model(NN_model, f'data_driven/models/data_driven_pairstrategy_model_every2ktimestep_{epochs}.pkl')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 577\u001b[0m, in \u001b[0;36mSPiFOL.train\u001b[1;34m(self, datatrain_input, datatrain_label, datatest_input, datatest_label, nIter)\u001b[0m\n\u001b[0;32m    574\u001b[0m batch_input_train \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(batch_input_train)\n\u001b[0;32m    575\u001b[0m batch_label_train \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(batch_label_train)\n\u001b[1;32m--> 577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitercount\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_label_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    581\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 559\u001b[0m, in \u001b[0;36mSPiFOL.step\u001b[1;34m(self, i, opt_state, batch_input, batch_label)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, opt_state, batch_input, batch_label):\n\u001b[0;32m    558\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(opt_state)\n\u001b[1;32m--> 559\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_update(i, grads, opt_state)\n",
      "    \u001b[1;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 551\u001b[0m, in \u001b[0;36mSPiFOL.loss_batches\u001b[1;34m(self, params, batch_input, batch_label)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch_input, batch_label):\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;66;03m# print(f'vmap agadi :{batch_input.shape, batch_label.shape}')\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     batch_loss  \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(batch_loss)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_loss\n",
      "    \u001b[1;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 538\u001b[0m, in \u001b[0;36mSPiFOL.loss_single\u001b[1;34m(self, params, batch_input, batch_label)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch_input, batch_label):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch input size inside loss_single is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_label\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m--> 538\u001b[0m     u_nn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperator_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# predicted or next value of the initial condition\u001b[39;00m\n\u001b[0;32m    540\u001b[0m     u_nn \u001b[38;5;241m=\u001b[39m u_nn\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m40\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;241m1\u001b[39m)    \n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_label size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_label\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 481\u001b[0m, in \u001b[0;36mSPiFOL.operator_net\u001b[1;34m(self, params, uk)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# # Flatten the output before passing to Dense layer\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# uk_flat = uk.reshape(uk.shape[0], -1)  # Flatten everything except batch dimension (i.e., (1, 40, 28, 28, 1) -> (1, 31360))\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Proceed with the network as usual\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNO\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 481\u001b[0m     O \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# N_apply expects a flattened input (1, 31360)\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     O \u001b[38;5;241m=\u001b[39m O\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape back to the desired output shape\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m O\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\example_libraries\\stax.py:307\u001b[0m, in \u001b[0;36mserial.<locals>.apply_fun\u001b[1;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m rngs \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(rng, nlayers) \u001b[38;5;28;01mif\u001b[39;00m rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m nlayers\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fun, param, rng \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(apply_funs, params, rngs):\n\u001b[1;32m--> 307\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m fun(param, inputs, rng\u001b[38;5;241m=\u001b[39mrng, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "Cell \u001b[1;32mIn[3], line 235\u001b[0m, in \u001b[0;36mPermute.<locals>.apply_fun\u001b[1;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_fun\u001b[39m(params, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# Ensure that the input is 5-dimensional before applying permutation\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 5-dimensional input, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape before permuting:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Apply permutation based on the order\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 5-dimensional input, got 2 dimensions."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule, exponential_decay\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "# from archs import FNOBlock3D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Complex Adam optimizer\n",
    "@optimizer\n",
    "def complex_adam(step_size, b1=0.9, b2=0.999, eps=1e-8):\n",
    "    \"\"\"Construct optimizer triple for complex-valued Adam.\"\"\"\n",
    "    step_size = make_schedule(step_size)\n",
    "\n",
    "    def init(x0):\n",
    "        m0 = jnp.zeros_like(x0)\n",
    "        v0 = jnp.zeros_like(x0)\n",
    "        return x0, m0, v0\n",
    "\n",
    "    def update(i, g, state):\n",
    "        x, m, v = state\n",
    "        g = jnp.conj(g)  # Complex conjugate\n",
    "        m = (1 - b1) * g + b1 * m  # First moment\n",
    "        v = (1 - b2) * jnp.real(jnp.conj(g) * g) + b2 * v  # Second moment\n",
    "        mhat = m / (1 - b1 ** (i + 1))  # Bias correction\n",
    "        vhat = v / (1 - b2 ** (i + 1))\n",
    "        x = x - step_size(i) * mhat / (jnp.sqrt(vhat) + eps)\n",
    "        return x, m, v\n",
    "\n",
    "    def get_params(state):\n",
    "        x, m, v = state\n",
    "        return x\n",
    "\n",
    "    return init, update, get_params\n",
    "\n",
    "\n",
    "# Define Dense Layer\n",
    "# def Dense(out_dim, W_init=jax.random.normal(key=jax.random.PRNGKey(42), shape=(31360, 64)), b_init=jax.random.normal(key=jax.random.PRNGKey(42), shape=(64,))):\n",
    "#     \"\"\"Layer constructor function for a dense (fully-connected) layer.\"\"\"\n",
    "#     # key = jax.random.PRNGKey(42)\n",
    "#     # # Initialize weights and biases with the key\n",
    "#     # W_init = jax.random.normal(key, shape=(31360, 64))  # Weight shape: (31360, 64)\n",
    "#     # b_init = jax.random.normal(key, shape=(64,))  \n",
    "#     def init_fun(rng, input_shape):\n",
    "#         output_shape = input_shape[:-1] + (out_dim,)\n",
    "#         k1, k2 = random.split(rng)\n",
    "#         W, b = W_init(k1, (input_shape[-1], out_dim)), b_init(k2, (out_dim,))\n",
    "#         return output_shape, (W, b)\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Before Dense - Input shape:\", inputs.shape)  # Should be (batch, 31360)\n",
    "\n",
    "#         W, b = params\n",
    "#         inputs = inputs.reshape(inputs.shape[0], -1)  # Flatten everything except batch\n",
    "#         print(\"Dense Layer - Flattened Input shape:\", inputs.shape)\n",
    "#         print(\"Dense Layer - Weight shape:\", W.shape)\n",
    "#         return jnp.dot(inputs, W) + b\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "\n",
    "# Define Dense Layer\n",
    "# this is rught for multiplication of imput and weight but i got error on einsum so i will use the below one\n",
    "def Dense(out_dim):\n",
    "    \"\"\"Layer constructor function for a dense (fully-connected) layer.\"\"\"\n",
    "    \n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = input_shape[:-1] + (out_dim,)\n",
    "        k1, k2 = random.split(rng)  # Split the random key to initialize weights and biases\n",
    "        \n",
    "        # Flatten the input shape to get the flattened size\n",
    "        input_dim = input_shape[-1] * input_shape[-2] * input_shape[-3] * input_shape[-4]  # Flattened size = 31360\n",
    "        print(\"Flattened input size:\", input_dim)\n",
    "        \n",
    "        # Initialize weights with the correct shape (31360, 64)\n",
    "        W = random.normal(k1, (input_dim, out_dim))  # Weight shape: (31360, 64)\n",
    "        print(\"Weight shape:\", W.shape)\n",
    "        b = random.normal(k2, (out_dim,))  # Bias shape: (64,)\n",
    "        \n",
    "        return output_shape, (W, b)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        print(\"Before Dense - Input shape:\", inputs.shape)  # Should be (batch, 31360)\n",
    "\n",
    "        W, b = params\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1)  # Flatten everything except batch dimension\n",
    "        print(\"Dense Layer - Flattened Input shape:\", inputs.shape)\n",
    "        print(\"Dense Layer - Weight shape:\", W.shape)\n",
    "        return jnp.dot(inputs, W) + b\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define FNO Block for Spatio-Temporal Data\n",
    "def FNOBlock3D(modes):\n",
    "    def compl_mul3d(input, weights):\n",
    "        # Corrected Einstein summation for 5D tensors\n",
    "        return jnp.einsum(\"bctvw, cotvw->botvw\", input, weights)\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        in_channels = input_shape[1]\n",
    "        W1 = random.normal(rng, (in_channels, in_channels, modes, modes, modes))\n",
    "        W2 = random.normal(rng, (in_channels, in_channels, modes, modes, modes))\n",
    "        return input_shape, (W1, W2)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        W1, W2 = params\n",
    "        x_ft = jnp.fft.rfftn(inputs, axes=(-3, -2, -1))  # FFT over time, height, width\n",
    "        out_ft = jnp.zeros_like(x_ft)\n",
    "        \n",
    "        # Apply weights to lower frequencies\n",
    "        out_ft = out_ft.at[:, :, :W1.shape[2], :W1.shape[3], :W1.shape[4]].set(\n",
    "            compl_mul3d(x_ft[:, :, :W1.shape[2], :W1.shape[3], :W1.shape[4]], W1)\n",
    "        )\n",
    "        # Apply weights to higher frequencies\n",
    "        out_ft = out_ft.at[:, :, -W2.shape[2]:, :W2.shape[3], :W2.shape[4]].set(\n",
    "            compl_mul3d(x_ft[:, :, -W2.shape[2]:, :W2.shape[3], :W2.shape[4]], W2)\n",
    "        )\n",
    "        return jnp.fft.irfftn(out_ft, s=inputs.shape[-3:])  # Inverse FFT\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "# define the einsum for permutation\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "            \n",
    "#             return (0, 4, 1, 2, 3)\n",
    "        \n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "#             return (0, 4, 1, 2, 3)\n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Input shape before permuting:\", inputs.shape)\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         print(\"Output shape after permuting:\", outputs.shape)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "#             return (0, 4, 1, 2, 3)\n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Input shape before permuting:\", inputs.shape)\n",
    "        \n",
    "#         # Apply permutation based on the order\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "# this function help to correct the permute function\n",
    "def Permute(order):\n",
    "    def permutation_indices(order):\n",
    "        if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "            return (0, 4, 1, 2, 3)\n",
    "        elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "            return (0, 2, 3, 4, 1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        idx = permutation_indices(order)\n",
    "        output_shape = tuple([input_shape[i] for i in idx])\n",
    "        return output_shape, ()\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        # Ensure that the input is 5-dimensional before applying permutation\n",
    "        if inputs.ndim != 5:\n",
    "            raise ValueError(f\"Expected 5-dimensional input, got {inputs.ndim} dimensions.\")\n",
    "        \n",
    "        print(\"Input shape before permuting:\", inputs.shape)\n",
    "        \n",
    "        # Apply permutation based on the order\n",
    "        outputs = jnp.einsum(order, inputs)\n",
    "        print(\"Output shape after permuting:\", outputs.shape)\n",
    "        return outputs\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "#             return (0, 4, 1, 2, 3)\n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Input shape before permuting:\", inputs.shape)\n",
    "        \n",
    "#         # Apply permutation based on the order\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         print(\"Output shape after permuting:\", outputs.shape)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # saving the parameter\n",
    "def save_model(model, filename):\n",
    "    # Save model parameters, architecture and optimizer state\n",
    "    save_dict = {\n",
    "        'arch': model.arch,\n",
    "        'N': model.N,\n",
    "        'lr': model.lr,\n",
    "        'eps': model.eps,\n",
    "        'pp2': model.pp2,\n",
    "        'qq2': model.qq2,\n",
    "        'dt': model.dt,\n",
    "        'L': model.L,\n",
    "        'h': model.h,\n",
    "        'x': model.x,\n",
    "        'y': model.y,\n",
    "        'params': jax.device_get(model.get_params(model.opt_state)),\n",
    "        'train_losses': model.train_losses,\n",
    "        'test_losses': model.test_losses,\n",
    "        'opt_state': jax.device_get(model.opt_state),  # Save optimizer state too\n",
    "    }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N//self.batch_size\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # Initialize the network based on architecture type\n",
    "        \n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(\n",
    "                random.PRNGKey(1234),\n",
    "                (-1, 40, N, N, 1)  # Input shape: (batch, time, height, width, channels)\n",
    "            )\n",
    "            \n",
    "        #     self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "        #     params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        # elif arch == 'modified_MLP':\n",
    "        #     self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "        #     params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "        )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    # def operator_net(self, params, uk):\n",
    "       \n",
    "    #     if self.arch == 'FNO':\n",
    "    #         print(\"Shape of uk before reshaping:\", uk.shape)  # Should be (40, 28, 28, 1)\n",
    "    #         # uk = uk.reshape(-1, 40, 1, self.N, self.N)  # Add channel and time dimensions\n",
    "    #         # Add batch dimension\n",
    "    #         uk = uk[None, :, :, :, :]  # Adding batch dimension (shape: (1, 40, 28, 28, 1))\n",
    "    #         print(\"Shape of uk after adding batch dimension:\", uk.shape)\n",
    "\n",
    "    #         uk = uk.reshape(-1, 40, self.N, self.N, 1)  # Add channel and time dimensions\n",
    "\n",
    "    #         O = self.N_apply(params, uk)\n",
    "    #         O = O.reshape(40, self.N, self.N, 1)\n",
    "    #         return O\n",
    "            \n",
    "    #     elif self.arch == 'MLP':\n",
    "    #         uk = uk.flatten()\n",
    "    #         O = self.N_apply(params, uk)  # Directly apply the network\n",
    "    #         O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "    #         return O\n",
    "    #     elif self.arch == 'modified_MLP':\n",
    "    #         uk = uk.flatten()\n",
    "    #         O = self.N_apply(params, uk)\n",
    "    #         O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "    #         return O\n",
    "    #     else:\n",
    "    #         raise ValueError(\"Unsupported architecture type!\")\n",
    "        \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        print(\"Shape of uk before reshaping:\", uk.shape)  # Should be (40, 28, 28, 1)\n",
    "        \n",
    "        # Add batch dimension to uk (it will be 5D after this)\n",
    "        uk = uk[None, :, :, :, :]  # Reshape to (1, 40, 28, 28, 1)\n",
    "        print(\"Shape of uk after adding batch dimension:\", uk.shape)  # Should be (1, 40, 28, 28, 1)\n",
    "        \n",
    "        # Define the permutation order (you can adjust the order based on your needs)\n",
    "        permute_order = \"ijklm->imjkl\"  # Example order\n",
    "        permute_fn = Permute(permute_order)\n",
    "        init_fun, apply_fun = permute_fn\n",
    "        \n",
    "        # Apply permutation\n",
    "        uk = apply_fun(None, uk)  # Apply the permutation function\n",
    "        print(\"Shape of uk after permuting:\", uk.shape)  # Check if the shape is correct\n",
    "\n",
    "        # # Flatten the output before passing to Dense layer\n",
    "        # uk_flat = uk.reshape(uk.shape[0], -1)  # Flatten everything except batch dimension (i.e., (1, 40, 28, 28, 1) -> (1, 31360))\n",
    "\n",
    "        # print(\"Flattened uk shape before passing to Dense:\", uk_flat.shape)  # Should be (1, 31360)\n",
    "\n",
    "\n",
    "        # Proceed with the network as usual\n",
    "        if self.arch == 'FNO':\n",
    "            O = self.N_apply(params, uk)  # N_apply expects a flattened input (1, 31360)\n",
    "            O = O.reshape(-1, 40, self.N, self.N, 1)  # Reshape back to the desired output shape\n",
    "            return O\n",
    "        \n",
    "        elif self.arch == 'MLP':\n",
    "            O = self.N_apply(params, uk)  # Apply MLP directly on the flattened input\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match the expected shape\n",
    "            return O\n",
    "        \n",
    "        elif self.arch == 'modified_MLP':\n",
    "            O = self.N_apply(params, uk)  # Apply modified MLP\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "\n",
    "      \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def allen_cahn_equation(self, uk):\n",
    "        \n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))\n",
    "        # print(\"Denominator shape:\", denominator.shape)\n",
    "\n",
    "        # Expand the denominator to match the shape of s_hat (28, 28, 1)\n",
    "        denominator = denominator[..., None]  # Add a third dimension to make the shape (28, 28, 1)\n",
    "        # print(\"Denominator shape after expansion:\", denominator.shape)\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        # print(\"Shape of s_hat (after fft2):\", s_hat.shape)\n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "        # print(\"Shape of v_hat (after division):\", v_hat.shape)\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        # print(\"Shape of uk (after ifft2):\", uk.shape)\n",
    "\n",
    "        uk = uk.reshape(self.N, self.N, 1)  # Reshaping to (N, N, 1)\n",
    "        # print(\"Shape of uk after reshaping:\", uk.shape)\n",
    "\n",
    "        # Return the real part\n",
    "        return jnp.real(uk)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, batch_input, batch_label):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        \n",
    "        print(f'batch input size inside loss_single is {batch_label.shape}') \n",
    "\n",
    "        u_nn = self.operator_net(params, batch_input) # predicted or next value of the initial condition\n",
    "\n",
    "        u_nn = u_nn.reshape(40,self.N, self.N, 1)    \n",
    "\n",
    "        print(f'batch_label size is {batch_label.shape}') \n",
    "       \n",
    "        datadriven_loss = jnp.mean((batch_label - u_nn) ** 2)\n",
    "        return datadriven_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch_input, batch_label):\n",
    "        # print(f'vmap agadi :{batch_input.shape, batch_label.shape}')\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0, 0))(params, batch_input, batch_label)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch_input, batch_label):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, batch_input, batch_label)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, datatrain_input, datatrain_label,  datatest_input, datatest_label, nIter=10000):\n",
    "        datainput_train_iter = iter(datatrain_input)\n",
    "        datatrain_label_iter = iter(datatrain_label)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch_input_train = next(datainput_train_iter)\n",
    "            batch_label_train= next(datatrain_label_iter)\n",
    "            batch_input_train = jnp.array(batch_input_train)\n",
    "            batch_label_train = jnp.array(batch_label_train)\n",
    "            \n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch_input_train, batch_label_train)\n",
    "\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss_train = self.loss_batches(params, batch_input_train, batch_label_train)\n",
    "                self.train_losses.append(loss_train)\n",
    "\n",
    "\n",
    "                # for testing\n",
    "                test_loss = []\n",
    "                for _ in range(len(datatest_input)):\n",
    "                    datatest_input_iter= iter(datatest_input)\n",
    "                    datatest_label_iter= iter(datatest_label)\n",
    "                    batch_input_test = next(datatest_input_iter)\n",
    "                    batch_label_test= next(datatest_label_iter)\n",
    "                    batch_input_test = jnp.array(batch_input_test)\n",
    "                    batch_label_test = jnp.array(batch_label_test)\n",
    "\n",
    "\n",
    "                    test_batch_loss = self.loss_batches(params, batch_input_test, batch_label_test)\n",
    "                    test_loss.append(test_batch_loss)\n",
    "                \n",
    "                mean_test_loss = jnp.mean(jnp.array(test_loss))\n",
    "                    \n",
    "                self.test_losses.append(mean_test_loss)\n",
    "            pbar.set_postfix({'train Loss': loss_train, 'test loss': mean_test_loss})\n",
    "\n",
    "\n",
    "    def pred(self, data_test, data_label):\n",
    "        # uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            # uk = self.allen_cahn_equation(item)\n",
    "            # cahn = eps**2\n",
    "            # uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # # Compute denominator in Fourier space\n",
    "            # denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # # Perform FFT calculations\n",
    "            # s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            # v_hat = s_hat / denominator  \n",
    "            # uk_ac = jfft.ifft2(v_hat)  \n",
    "            # uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            # uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        # uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(data_label, (data_label.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_nnetwork\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.title(\"data_driven_training\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.05 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.0001 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "# print(f'pp2 shape:{pp2.shape}')\n",
    "# print(f'qq2 shape:{qq2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "\n",
    "# Define FNO layers for Spatio-Temporal Data\n",
    "# fno_layers = [\n",
    "#     Dense(64),\n",
    "#     Permute(\"ijklm->imjkl\"),\n",
    "#     FNOBlock3D(15),\n",
    "#     Gelu,  # Use Gelu() instead of Gelu\n",
    "#     FNOBlock3D(15),\n",
    "#     Gelu,  # Use Gelu() instead of Gelu\n",
    "#     FNOBlock3D(15),\n",
    "#     Permute(\"ijklm->iklmj\"),\n",
    "#     Dense(128),\n",
    "#     Gelu,  # Use Gelu() instead of Gelu\n",
    "#     Dense(1),\n",
    "# ]\n",
    "\n",
    "fno_layers = [\n",
    "    Permute(\"ijklm->imjkl\"),  # (batch, time, H, W, C) → (batch, C, time, H, W)\n",
    "    FNOBlock3D(15),\n",
    "    Gelu(),\n",
    "    FNOBlock3D(15),\n",
    "    Gelu(),\n",
    "    FNOBlock3D(15),\n",
    "    Permute(\"ijklm->iklmj\"),  # (batch, C, time, H, W) → (batch, time, H, W, C)\n",
    "    Dense(128),\n",
    "    Gelu(),\n",
    "    Dense(1),\n",
    "]\n",
    "\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 10\n",
    "\n",
    "data = np.load(\"../data_driven/data/driven_data_28x28_1ksample_41timestep_every1kiter.npy\")\n",
    "\n",
    "# pairing the dataset ex 0_timesteps -> 4k_timestep, 4k_timestep -> 8k_timesteps so on.\n",
    "data_input = data[:, :-1, :, :]\n",
    "data_label = data[:, 1:, :, :]\n",
    "# print('data_input_shape:', data_input.shape)\n",
    "# print('data_label_shape:', data_label.shape)\n",
    "# print(f'dataset ko shape:{data_input.shape, data_label.shape}')\n",
    "\n",
    "# data_plot_input = data_input[610][0]# data_plot_label = data_label[610][9]\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 4))  # 11 time steps\n",
    "# print(axes)\n",
    "# axes[0].imshow(data_plot_input)\n",
    "# axes[1].imshow(data_plot_label)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Generate the data trainig samples\n",
    "data_input_reshape = data_input.reshape(-1, 40, N, N, 1 )\n",
    "data_label_reshape = data_label.reshape(-1, 40, N, N, 1) # label is the gt here \n",
    "# print(f'dataset ko shape after reshape:{data_input_reshape.shape, data_label_reshape.shape}')\n",
    "\n",
    "# Split the dataset\n",
    "train_input, test_input, train_label, test_label = train_test_split(\n",
    "    data_input_reshape, data_label_reshape, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input.shape}\")\n",
    "# print(f\"Test Input Shape: {test_input.shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label.shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label.shape}\")\n",
    "\n",
    "train_input_batch = DataGenerator(train_input, batch_size=20)\n",
    "test_input_batch = DataGenerator(test_input, batch_size=20)\n",
    "train_label_batch = DataGenerator(train_label, batch_size=20)\n",
    "test_label_batch = DataGenerator(test_label, batch_size=20)\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input_batch[0].shape}\")\n",
    "# print(f\"Test Input Shape: {test_input_batch[0].shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label_batch[0].shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label_batch[0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  # Specify the directory where y want to save the data\n",
    "# save_dir = './data_driven//'\n",
    "\n",
    "#      # Ensure the directory exists, create it if not\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "print(\"Shape of batch_input_train:\", train_input_batch[0].shape)\n",
    "print(\"Shape of batch_label_train:\", train_label_batch[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "NN_model.train(train_input_batch, train_label_batch,  test_input_batch, test_label_batch, nIter = epochs)\n",
    "\n",
    "# Now call loss function\n",
    "# loss = NN_model.loss_batches(params, batch_input, batch_label)\n",
    "# r2, mse, l2_rel, u_pred = NN_model.pred(test_input, test_label)\n",
    "# print(f'r2:{r2},mse : {mse}, l2_rel : {l2_rel}')\n",
    "\n",
    "# NN_model.plot_losses(f'data_driven/data_driven_plot/data_driven_every2ktimesiter_training_log_iter_{epochs}.png')\n",
    "\n",
    "\n",
    "# save_model(NN_model, f'data_driven/models/data_driven_pairstrategy_model_every2ktimestep_{epochs}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch_input_train: (20, 40, 28, 28, 1)\n",
      "Shape of batch_label_train: (20, 40, 28, 28, 1)\n",
      "Flattened input size: 31360\n",
      "Weight shape: (31360, 64)\n",
      "Flattened input size: 2007040\n",
      "Weight shape: (2007040, 128)\n",
      "Flattened input size: 4014080\n",
      "Weight shape: (4014080, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input size inside loss_single is (40, 28, 28, 1)\n",
      "Shape of uk before reshaping: (40, 28, 28, 1)\n",
      "Shape of uk after adding batch dimension: (1, 40, 28, 28, 1)\n",
      "Input shape before permuting: (1, 40, 28, 28, 1)\n",
      "Output shape after permuting: (1, 1, 40, 28, 28)\n",
      "Shape of uk after permuting: (1, 1, 40, 28, 28)\n",
      "Flattened uk shape before Dense: (1, 31360)\n",
      "Before Dense - Input shape: (1, 31360)\n",
      "Dense Layer - Flattened Input shape: (1, 31360)\n",
      "Dense Layer - Weight shape: (31360, 64)\n",
      "Input shape before permuting: (1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Einstein sum subscript 'ijklm' does not contain the correct number of indices for operand 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 791\u001b[0m\n",
      "\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m#    # Initialize and train the model\u001b[39;00m\n",
      "\u001b[0;32m    790\u001b[0m NN_model \u001b[38;5;241m=\u001b[39m SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m--> 791\u001b[0m \u001b[43mNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    793\u001b[0m \u001b[38;5;66;03m# Now call loss function\u001b[39;00m\n",
      "\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# loss = NN_model.loss_batches(params, batch_input, batch_label)\u001b[39;00m\n",
      "\u001b[0;32m    795\u001b[0m \u001b[38;5;66;03m# r2, mse, l2_rel, u_pred = NN_model.pred(test_input, test_label)\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    800\u001b[0m \n",
      "\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# save_model(NN_model, f'data_driven/models/data_driven_pairstrategy_model_every2ktimestep_{epochs}.pkl')\u001b[39;00m\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 574\u001b[0m, in \u001b[0;36mSPiFOL.train\u001b[1;34m(self, datatrain_input, datatrain_label, datatest_input, datatest_label, nIter)\u001b[0m\n",
      "\u001b[0;32m    571\u001b[0m batch_input_train \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(batch_input_train)\n",
      "\u001b[0;32m    572\u001b[0m batch_label_train \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(batch_label_train)\n",
      "\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitercount\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_label_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m    578\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 556\u001b[0m, in \u001b[0;36mSPiFOL.step\u001b[1;34m(self, i, opt_state, batch_input, batch_label)\u001b[0m\n",
      "\u001b[0;32m    553\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n",
      "\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, opt_state, batch_input, batch_label):\n",
      "\u001b[0;32m    555\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(opt_state)\n",
      "\u001b[1;32m--> 556\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_update(i, grads, opt_state)\n",
      "\n",
      "    \u001b[1;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 548\u001b[0m, in \u001b[0;36mSPiFOL.loss_batches\u001b[1;34m(self, params, batch_input, batch_label)\u001b[0m\n",
      "\u001b[0;32m    544\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n",
      "\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch_input, batch_label):\n",
      "\u001b[0;32m    546\u001b[0m     \u001b[38;5;66;03m# print(f'vmap agadi :{batch_input.shape, batch_label.shape}')\u001b[39;00m\n",
      "\u001b[1;32m--> 548\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    549\u001b[0m     batch_loss  \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(batch_loss)\n",
      "\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_loss\n",
      "\n",
      "    \u001b[1;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 535\u001b[0m, in \u001b[0;36mSPiFOL.loss_single\u001b[1;34m(self, params, batch_input, batch_label)\u001b[0m\n",
      "\u001b[0;32m    529\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n",
      "\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch_input, batch_label):\n",
      "\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\u001b[39;00m\n",
      "\u001b[0;32m    533\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch input size inside loss_single is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_label\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;32m--> 535\u001b[0m     u_nn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperator_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# predicted or next value of the initial condition\u001b[39;00m\n",
      "\u001b[0;32m    537\u001b[0m     u_nn \u001b[38;5;241m=\u001b[39m u_nn\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m40\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;241m1\u001b[39m)    \n",
      "\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_label size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_label\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 480\u001b[0m, in \u001b[0;36mSPiFOL.operator_net\u001b[1;34m(self, params, uk)\u001b[0m\n",
      "\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# Proceed with the network as usual\u001b[39;00m\n",
      "\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNO\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;32m--> 480\u001b[0m     O \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muk_flat\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    481\u001b[0m     O \u001b[38;5;241m=\u001b[39m O\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m O\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\example_libraries\\stax.py:307\u001b[0m, in \u001b[0;36mserial.<locals>.apply_fun\u001b[1;34m(params, inputs, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    305\u001b[0m rngs \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(rng, nlayers) \u001b[38;5;28;01mif\u001b[39;00m rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m nlayers\n",
      "\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fun, param, rng \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(apply_funs, params, rngs):\n",
      "\u001b[1;32m--> 307\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m fun(param, inputs, rng\u001b[38;5;241m=\u001b[39mrng, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\n",
      "Cell \u001b[1;32mIn[1], line 265\u001b[0m, in \u001b[0;36mPermute.<locals>.apply_fun\u001b[1;34m(params, inputs, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape before permuting:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Apply permutation based on the order\u001b[39;00m\n",
      "\u001b[1;32m--> 265\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape after permuting:\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:5181\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(subscripts, out, optimize, precision, preferred_element_type, _dot_general, *operands)\u001b[0m\n",
      "\u001b[0;32m   5179\u001b[0m   contract_path \u001b[38;5;241m=\u001b[39m _poly_einsum_handlers\u001b[38;5;241m.\u001b[39mget(ty, _default_poly_einsum_handler)\n",
      "\u001b[0;32m   5180\u001b[0m \u001b[38;5;66;03m# using einsum_call=True here is an internal api for opt_einsum... sorry\u001b[39;00m\n",
      "\u001b[1;32m-> 5181\u001b[0m operands, contractions \u001b[38;5;241m=\u001b[39m \u001b[43mcontract_path\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   5182\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_blas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   5184\u001b[0m contractions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((a, \u001b[38;5;28mfrozenset\u001b[39m(b), c) \u001b[38;5;28;01mfor\u001b[39;00m a, b, c, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;129;01min\u001b[39;00m contractions)\n",
      "\u001b[0;32m   5186\u001b[0m einsum \u001b[38;5;241m=\u001b[39m jit(_einsum, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m), inline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\opt_einsum\\contract.py:312\u001b[0m, in \u001b[0;36mcontract_path\u001b[1;34m(subscripts, use_blas, optimize, memory_limit, shapes, *operands, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    309\u001b[0m sh \u001b[38;5;241m=\u001b[39m input_shapes[tnum]\n",
      "\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sh) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(term):\n",
      "\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEinstein sum subscript \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_list[tnum]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not contain the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect number of indices for operand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    315\u001b[0m     )\n",
      "\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnum, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(term):\n",
      "\u001b[0;32m    317\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sh[cnum])\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Einstein sum subscript 'ijklm' does not contain the correct number of indices for operand 0."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "# from archs import FNOBlock3D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Complex Adam optimizer\n",
    "@optimizer\n",
    "def complex_adam(step_size, b1=0.9, b2=0.999, eps=1e-8):\n",
    "    \"\"\"Construct optimizer triple for complex-valued Adam.\"\"\"\n",
    "    step_size = make_schedule(step_size)\n",
    "\n",
    "    def init(x0):\n",
    "        m0 = jnp.zeros_like(x0)\n",
    "        v0 = jnp.zeros_like(x0)\n",
    "        return x0, m0, v0\n",
    "\n",
    "    def update(i, g, state):\n",
    "        x, m, v = state\n",
    "        g = jnp.conj(g)  # Complex conjugate\n",
    "        m = (1 - b1) * g + b1 * m  # First moment\n",
    "        v = (1 - b2) * jnp.real(jnp.conj(g) * g) + b2 * v  # Second moment\n",
    "        mhat = m / (1 - b1 ** (i + 1))  # Bias correction\n",
    "        vhat = v / (1 - b2 ** (i + 1))\n",
    "        x = x - step_size(i) * mhat / (jnp.sqrt(vhat) + eps)\n",
    "        return x, m, v\n",
    "\n",
    "    def get_params(state):\n",
    "        x, m, v = state\n",
    "        return x\n",
    "\n",
    "    return init, update, get_params\n",
    "\n",
    "\n",
    "# Define Dense Layer\n",
    "# def Dense(out_dim, W_init=jax.random.normal(key=jax.random.PRNGKey(42), shape=(31360, 64)), b_init=jax.random.normal(key=jax.random.PRNGKey(42), shape=(64,))):\n",
    "#     \"\"\"Layer constructor function for a dense (fully-connected) layer.\"\"\"\n",
    "#     # key = jax.random.PRNGKey(42)\n",
    "#     # # Initialize weights and biases with the key\n",
    "#     # W_init = jax.random.normal(key, shape=(31360, 64))  # Weight shape: (31360, 64)\n",
    "#     # b_init = jax.random.normal(key, shape=(64,))  \n",
    "#     def init_fun(rng, input_shape):\n",
    "#         output_shape = input_shape[:-1] + (out_dim,)\n",
    "#         k1, k2 = random.split(rng)\n",
    "#         W, b = W_init(k1, (input_shape[-1], out_dim)), b_init(k2, (out_dim,))\n",
    "#         return output_shape, (W, b)\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Before Dense - Input shape:\", inputs.shape)  # Should be (batch, 31360)\n",
    "\n",
    "#         W, b = params\n",
    "#         inputs = inputs.reshape(inputs.shape[0], -1)  # Flatten everything except batch\n",
    "#         print(\"Dense Layer - Flattened Input shape:\", inputs.shape)\n",
    "#         print(\"Dense Layer - Weight shape:\", W.shape)\n",
    "#         return jnp.dot(inputs, W) + b\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "\n",
    "# Define Dense Layer\n",
    "# this is rught for multiplication of imput and weight but i got error on einsum so i will use the below one\n",
    "def Dense(out_dim):\n",
    "    \"\"\"Layer constructor function for a dense (fully-connected) layer.\"\"\"\n",
    "    \n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = input_shape[:-1] + (out_dim,)\n",
    "        k1, k2 = random.split(rng)  # Split the random key to initialize weights and biases\n",
    "        \n",
    "        # Flatten the input shape to get the flattened size\n",
    "        input_dim = input_shape[-1] * input_shape[-2] * input_shape[-3] * input_shape[-4]  # Flattened size = 31360\n",
    "        print(\"Flattened input size:\", input_dim)\n",
    "        \n",
    "        # Initialize weights with the correct shape (31360, 64)\n",
    "        W = random.normal(k1, (input_dim, out_dim))  # Weight shape: (31360, 64)\n",
    "        print(\"Weight shape:\", W.shape)\n",
    "        b = random.normal(k2, (out_dim,))  # Bias shape: (64,)\n",
    "        \n",
    "        return output_shape, (W, b)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        print(\"Before Dense - Input shape:\", inputs.shape)  # Should be (batch, 31360)\n",
    "\n",
    "        W, b = params\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1)  # Flatten everything except batch dimension\n",
    "        print(\"Dense Layer - Flattened Input shape:\", inputs.shape)\n",
    "        print(\"Dense Layer - Weight shape:\", W.shape)\n",
    "        return jnp.dot(inputs, W) + b\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define FNO Block for Spatio-Temporal Data\n",
    "def FNOBlock3D(modes):\n",
    "    def compl_mul3d(input, weights):\n",
    "        # Corrected Einstein summation for 5D tensors\n",
    "        return jnp.einsum(\"bctvw, cotvw->botvw\", input, weights)\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        in_channels = input_shape[1]\n",
    "        W1 = random.normal(rng, (in_channels, in_channels, modes, modes, modes))\n",
    "        W2 = random.normal(rng, (in_channels, in_channels, modes, modes, modes))\n",
    "        return input_shape, (W1, W2)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        W1, W2 = params\n",
    "        x_ft = jnp.fft.rfftn(inputs, axes=(-3, -2, -1))  # FFT over time, height, width\n",
    "        out_ft = jnp.zeros_like(x_ft)\n",
    "        \n",
    "        # Apply weights to lower frequencies\n",
    "        out_ft = out_ft.at[:, :, :W1.shape[2], :W1.shape[3], :W1.shape[4]].set(\n",
    "            compl_mul3d(x_ft[:, :, :W1.shape[2], :W1.shape[3], :W1.shape[4]], W1)\n",
    "        )\n",
    "        # Apply weights to higher frequencies\n",
    "        out_ft = out_ft.at[:, :, -W2.shape[2]:, :W2.shape[3], :W2.shape[4]].set(\n",
    "            compl_mul3d(x_ft[:, :, -W2.shape[2]:, :W2.shape[3], :W2.shape[4]], W2)\n",
    "        )\n",
    "        return jnp.fft.irfftn(out_ft, s=inputs.shape[-3:])  # Inverse FFT\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "# define the einsum for permutation\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "            \n",
    "#             return (0, 4, 1, 2, 3)\n",
    "        \n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "#             return (0, 4, 1, 2, 3)\n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Input shape before permuting:\", inputs.shape)\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         print(\"Output shape after permuting:\", outputs.shape)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "#             return (0, 4, 1, 2, 3)\n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         print(\"Input shape before permuting:\", inputs.shape)\n",
    "        \n",
    "#         # Apply permutation based on the order\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "# this function help to correct the permute function\n",
    "# def Permute(order):\n",
    "#     def permutation_indices(order):\n",
    "#         if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "#             return (0, 4, 1, 2, 3)\n",
    "#         elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "#             return (0, 2, 3, 4, 1)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def init_fun(rng, input_shape):\n",
    "#         idx = permutation_indices(order)\n",
    "#         output_shape = tuple([input_shape[i] for i in idx])\n",
    "#         return output_shape, ()\n",
    "\n",
    "#     def apply_fun(params, inputs, **kwargs):\n",
    "#         # Ensure that the input is 5-dimensional before applying permutation\n",
    "#         if inputs.ndim != 5:\n",
    "#             raise ValueError(f\"Expected 5-dimensional input, got {inputs.ndim} dimensions.\")\n",
    "        \n",
    "#         print(\"Input shape before permuting:\", inputs.shape)\n",
    "        \n",
    "#         # Apply permutation based on the order\n",
    "#         outputs = jnp.einsum(order, inputs)\n",
    "#         print(\"Output shape after permuting:\", outputs.shape)\n",
    "#         return outputs\n",
    "\n",
    "#     return init_fun, apply_fun\n",
    "\n",
    "\n",
    "def Permute(order):\n",
    "    def permutation_indices(order):\n",
    "        if order == \"ijklm->imjkl\":  # (batch, time, height, width, channels) → (batch, channels, time, height, width)\n",
    "            return (0, 4, 1, 2, 3)\n",
    "        elif order == \"ijklm->iklmj\":  # (batch, channels, time, height, width) → (batch, time, height, width, channels)\n",
    "            return (0, 2, 3, 4, 1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def init_fun(rng, input_shape):\n",
    "        idx = permutation_indices(order)\n",
    "        output_shape = tuple([input_shape[i] for i in idx])\n",
    "        return output_shape, ()\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        print(\"Input shape before permuting:\", inputs.shape)\n",
    "        \n",
    "        # Apply permutation based on the order\n",
    "        outputs = jnp.einsum(order, inputs)\n",
    "        print(\"Output shape after permuting:\", outputs.shape)\n",
    "        return outputs\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # saving the parameter\n",
    "def save_model(model, filename):\n",
    "    # Save model parameters, architecture and optimizer state\n",
    "    save_dict = {\n",
    "        'arch': model.arch,\n",
    "        'N': model.N,\n",
    "        'lr': model.lr,\n",
    "        'eps': model.eps,\n",
    "        'pp2': model.pp2,\n",
    "        'qq2': model.qq2,\n",
    "        'dt': model.dt,\n",
    "        'L': model.L,\n",
    "        'h': model.h,\n",
    "        'x': model.x,\n",
    "        'y': model.y,\n",
    "        'params': jax.device_get(model.get_params(model.opt_state)),\n",
    "        'train_losses': model.train_losses,\n",
    "        'test_losses': model.test_losses,\n",
    "        'opt_state': jax.device_get(model.opt_state),  # Save optimizer state too\n",
    "    }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N//self.batch_size\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # Initialize the network based on architecture type\n",
    "        \n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(\n",
    "                random.PRNGKey(1234),\n",
    "                (-1, 40, N, N, 1)  # Input shape: (batch, time, height, width, channels)\n",
    "            )\n",
    "            \n",
    "        #     self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "        #     params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        # elif arch == 'modified_MLP':\n",
    "        #     self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "        #     params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "            )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    # def operator_net(self, params, uk):\n",
    "       \n",
    "    #     if self.arch == 'FNO':\n",
    "    #         print(\"Shape of uk before reshaping:\", uk.shape)  # Should be (40, 28, 28, 1)\n",
    "    #         # uk = uk.reshape(-1, 40, 1, self.N, self.N)  # Add channel and time dimensions\n",
    "    #         # Add batch dimension\n",
    "    #         uk = uk[None, :, :, :, :]  # Adding batch dimension (shape: (1, 40, 28, 28, 1))\n",
    "    #         print(\"Shape of uk after adding batch dimension:\", uk.shape)\n",
    "\n",
    "    #         uk = uk.reshape(-1, 40, self.N, self.N, 1)  # Add channel and time dimensions\n",
    "\n",
    "    #         O = self.N_apply(params, uk)\n",
    "    #         O = O.reshape(40, self.N, self.N, 1)\n",
    "    #         return O\n",
    "            \n",
    "    #     elif self.arch == 'MLP':\n",
    "    #         uk = uk.flatten()\n",
    "    #         O = self.N_apply(params, uk)  # Directly apply the network\n",
    "    #         O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "    #         return O\n",
    "    #     elif self.arch == 'modified_MLP':\n",
    "    #         uk = uk.flatten()\n",
    "    #         O = self.N_apply(params, uk)\n",
    "    #         O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "    #         return O\n",
    "    #     else:\n",
    "    #         raise ValueError(\"Unsupported architecture type!\")\n",
    "        \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        print(\"Shape of uk before reshaping:\", uk.shape)  # Should be (40, 28, 28, 1)\n",
    "        \n",
    "        # Add batch dimension to uk (it will be 5D after this)\n",
    "        uk = uk[None, :, :, :, :]  # Reshape to (1, 40, 28, 28, 1)\n",
    "        print(\"Shape of uk after adding batch dimension:\", uk.shape)  # Should be (1, 40, 28, 28, 1)\n",
    "        \n",
    "        # Define the permutation order (you can adjust the order based on your needs)\n",
    "        permute_order = \"ijklm->imjkl\"  # Example order\n",
    "        permute_fn = Permute(permute_order)\n",
    "        init_fun, apply_fun = permute_fn\n",
    "        \n",
    "        # Apply permutation\n",
    "        uk = apply_fun(None, uk)  # Apply the permutation function\n",
    "        print(\"Shape of uk after permuting:\", uk.shape)  # Check if the shape is correct\n",
    "\n",
    "        # Flatten the output before passing to Dense layer\n",
    "        uk_flat = uk.reshape(uk.shape[0], -1)  # Flatten everything except batch dimension (i.e., (1, 40, 28, 28, 1) -> (1, 31360))\n",
    "\n",
    "        print(\"Flattened uk shape before Dense:\", uk_flat.shape)  # Should be (1, 31360)\n",
    "\n",
    "        # Proceed with the network as usual\n",
    "        if self.arch == 'FNO':\n",
    "            O = self.N_apply(params, uk_flat)\n",
    "            O = O.reshape(40, self.N, self.N, 1)\n",
    "            return O\n",
    "        \n",
    "        elif self.arch == 'MLP':\n",
    "            O = self.N_apply(params, uk_flat)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        elif self.arch == 'modified_MLP':\n",
    "            O = self.N_apply(params, uk_flat)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "\n",
    "      \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def allen_cahn_equation(self, uk):\n",
    "        \n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))\n",
    "        # print(\"Denominator shape:\", denominator.shape)\n",
    "\n",
    "        # Expand the denominator to match the shape of s_hat (28, 28, 1)\n",
    "        denominator = denominator[..., None]  # Add a third dimension to make the shape (28, 28, 1)\n",
    "        # print(\"Denominator shape after expansion:\", denominator.shape)\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        # print(\"Shape of s_hat (after fft2):\", s_hat.shape)\n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "        # print(\"Shape of v_hat (after division):\", v_hat.shape)\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        # print(\"Shape of uk (after ifft2):\", uk.shape)\n",
    "\n",
    "        uk = uk.reshape(self.N, self.N, 1)  # Reshaping to (N, N, 1)\n",
    "        # print(\"Shape of uk after reshaping:\", uk.shape)\n",
    "\n",
    "        # Return the real part\n",
    "        return jnp.real(uk)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, batch_input, batch_label):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        \n",
    "        print(f'batch input size inside loss_single is {batch_label.shape}') \n",
    "\n",
    "        u_nn = self.operator_net(params, batch_input) # predicted or next value of the initial condition\n",
    "\n",
    "        u_nn = u_nn.reshape(40,self.N, self.N, 1)    \n",
    "\n",
    "        print(f'batch_label size is {batch_label.shape}') \n",
    "       \n",
    "        datadriven_loss = jnp.mean((batch_label - u_nn) ** 2)\n",
    "        return datadriven_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch_input, batch_label):\n",
    "        # print(f'vmap agadi :{batch_input.shape, batch_label.shape}')\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0, 0))(params, batch_input, batch_label)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch_input, batch_label):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, batch_input, batch_label)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, datatrain_input, datatrain_label,  datatest_input, datatest_label, nIter=10000):\n",
    "        datainput_train_iter = iter(datatrain_input)\n",
    "        datatrain_label_iter = iter(datatrain_label)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch_input_train = next(datainput_train_iter)\n",
    "            batch_label_train= next(datatrain_label_iter)\n",
    "            batch_input_train = jnp.array(batch_input_train)\n",
    "            batch_label_train = jnp.array(batch_label_train)\n",
    "            \n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch_input_train, batch_label_train)\n",
    "\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss_train = self.loss_batches(params, batch_input_train, batch_label_train)\n",
    "                self.train_losses.append(loss_train)\n",
    "\n",
    "\n",
    "                # for testing\n",
    "                test_loss = []\n",
    "                for _ in range(len(datatest_input)):\n",
    "                    datatest_input_iter= iter(datatest_input)\n",
    "                    datatest_label_iter= iter(datatest_label)\n",
    "                    batch_input_test = next(datatest_input_iter)\n",
    "                    batch_label_test= next(datatest_label_iter)\n",
    "                    batch_input_test = jnp.array(batch_input_test)\n",
    "                    batch_label_test = jnp.array(batch_label_test)\n",
    "\n",
    "\n",
    "                    test_batch_loss = self.loss_batches(params, batch_input_test, batch_label_test)\n",
    "                    test_loss.append(test_batch_loss)\n",
    "                \n",
    "                mean_test_loss = jnp.mean(jnp.array(test_loss))\n",
    "                    \n",
    "                self.test_losses.append(mean_test_loss)\n",
    "            pbar.set_postfix({'train Loss': loss_train, 'test loss': mean_test_loss})\n",
    "\n",
    "\n",
    "    def pred(self, data_test, data_label):\n",
    "        # uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            # uk = self.allen_cahn_equation(item)\n",
    "            # cahn = eps**2\n",
    "            # uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # # Compute denominator in Fourier space\n",
    "            # denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # # Perform FFT calculations\n",
    "            # s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            # v_hat = s_hat / denominator  \n",
    "            # uk_ac = jfft.ifft2(v_hat)  \n",
    "            # uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            # uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        # uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(data_label, (data_label.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_nnetwork\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.title(\"data_driven_training\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.05 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.0001 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "# print(f'pp2 shape:{pp2.shape}')\n",
    "# print(f'qq2 shape:{qq2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "\n",
    "# Define FNO layers for Spatio-Temporal Data\n",
    "fno_layers = [\n",
    "    Dense(64),\n",
    "    Permute(\"ijklm->imjkl\"),\n",
    "    FNOBlock3D(15),\n",
    "    Gelu,  # Use Gelu() instead of Gelu\n",
    "    FNOBlock3D(15),\n",
    "    Gelu,  # Use Gelu() instead of Gelu\n",
    "    FNOBlock3D(15),\n",
    "    Permute(\"ijklm->iklmj\"),\n",
    "    Dense(128),\n",
    "    Gelu,  # Use Gelu() instead of Gelu\n",
    "    Dense(1),\n",
    "]\n",
    "\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 10\n",
    "\n",
    "data = np.load(\"../data_driven/data/driven_data_28x28_1ksample_41timestep_every1kiter.npy\")\n",
    "\n",
    "# pairing the dataset ex 0_timesteps -> 4k_timestep, 4k_timestep -> 8k_timesteps so on.\n",
    "data_input = data[:, :-1, :, :]\n",
    "data_label = data[:, 1:, :, :]\n",
    "# print('data_input_shape:', data_input.shape)\n",
    "# print('data_label_shape:', data_label.shape)\n",
    "# print(f'dataset ko shape:{data_input.shape, data_label.shape}')\n",
    "\n",
    "# data_plot_input = data_input[610][0]# data_plot_label = data_label[610][9]\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 4))  # 11 time steps\n",
    "# print(axes)\n",
    "# axes[0].imshow(data_plot_input)\n",
    "# axes[1].imshow(data_plot_label)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Generate the data trainig samples\n",
    "data_input_reshape = data_input.reshape(-1, 40, N, N, 1 )\n",
    "data_label_reshape = data_label.reshape(-1, 40, N, N, 1) # label is the gt here \n",
    "# print(f'dataset ko shape after reshape:{data_input_reshape.shape, data_label_reshape.shape}')\n",
    "\n",
    "# Split the dataset\n",
    "train_input, test_input, train_label, test_label = train_test_split(\n",
    "    data_input_reshape, data_label_reshape, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input.shape}\")\n",
    "# print(f\"Test Input Shape: {test_input.shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label.shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label.shape}\")\n",
    "\n",
    "train_input_batch = DataGenerator(train_input, batch_size=20)\n",
    "test_input_batch = DataGenerator(test_input, batch_size=20)\n",
    "train_label_batch = DataGenerator(train_label, batch_size=20)\n",
    "test_label_batch = DataGenerator(test_label, batch_size=20)\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input_batch[0].shape}\")\n",
    "# print(f\"Test Input Shape: {test_input_batch[0].shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label_batch[0].shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label_batch[0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  # Specify the directory where y want to save the data\n",
    "# save_dir = './data_driven//'\n",
    "\n",
    "#      # Ensure the directory exists, create it if not\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "print(\"Shape of batch_input_train:\", train_input_batch[0].shape)\n",
    "print(\"Shape of batch_label_train:\", train_label_batch[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "NN_model.train(train_input_batch, train_label_batch,  test_input_batch, test_label_batch, nIter = epochs)\n",
    "\n",
    "# Now call loss function\n",
    "# loss = NN_model.loss_batches(params, batch_input, batch_label)\n",
    "# r2, mse, l2_rel, u_pred = NN_model.pred(test_input, test_label)\n",
    "# print(f'r2:{r2},mse : {mse}, l2_rel : {l2_rel}')\n",
    "\n",
    "# NN_model.plot_losses(f'data_driven/data_driven_plot/data_driven_every2ktimesiter_training_log_iter_{epochs}.png')\n",
    "\n",
    "\n",
    "# save_model(NN_model, f'data_driven/models/data_driven_pairstrategy_model_every2ktimestep_{epochs}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
