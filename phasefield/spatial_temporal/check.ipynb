{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input size inside loss_single is (28, 28, 1)\n",
      "batch input size inside loss_single is (28, 28, 1)\n",
      "Shape of uk before reshaping (inside operator net): (28, 28, 1)\n",
      "Shape of uk after reshaping (inside operator net): (28, 28, 1)\n",
      "Shape of uk after N-apply  (inside operator net): (1, 28, 28, 1)\n",
      "Shape of uk after N-apply and reshape (inside operator net): (28, 28, 1)\n",
      "batch input size inside loss_single (network prediction) is (28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\lax\\lax.py:2785: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n",
      " 40%|████      | 2/5 [00:44<01:06, 22.11s/it, train Loss=58.33566, test loss=60.12147] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 499\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Print shapes to verify\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# print(f\"Train Input Shape: {train_input_batch[0].shape}\")\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# print(f\"Test Input Shape: {test_input_batch[0].shape}\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m \n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m#    # Initialize and train the model\u001b[39;00m\n\u001b[0;32m    498\u001b[0m NN_model \u001b[38;5;241m=\u001b[39m SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 499\u001b[0m \u001b[43mNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# Now call loss function\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# loss = NN_model.loss_batches(params, batch_input, batch_label)\u001b[39;00m\n\u001b[0;32m    503\u001b[0m r2, mse, l2_rel, u_pred \u001b[38;5;241m=\u001b[39m NN_model\u001b[38;5;241m.\u001b[39mpred(test_input, test_label)\n",
      "Cell \u001b[1;32mIn[2], line 284\u001b[0m, in \u001b[0;36mSPiFOL.train\u001b[1;34m(self, datatrain_input, datatrain_label, datatest_input, datatest_label, nIter)\u001b[0m\n\u001b[0;32m    279\u001b[0m pbar \u001b[38;5;241m=\u001b[39m trange(nIter)  \u001b[38;5;66;03m# Progress bar for total iterations\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m--> 284\u001b[0m     batch_input_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatainput_train_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     batch_label_train\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(datatrain_label_iter)\n\u001b[0;32m    286\u001b[0m     batch_input_train \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(batch_input_train)\n",
      "Cell \u001b[1;32mIn[2], line 103\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerate one batch of data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey, subkey \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_generation(subkey)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\random.py:278\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(key, num)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a PRNG key into `num` new keys by adding a leading axis.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  An array-like object of `num` new PRNG keys.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m typed_key, wrapped \u001b[38;5;241m=\u001b[39m _check_prng_key(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(wrapped, \u001b[43m_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\random.py:264\u001b[0m, in \u001b[0;36m_split\u001b[1;34m(key, num)\u001b[0m\n\u001b[0;32m    261\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit accepts a single key, but was given a key array of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(num) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num, Sequence) \u001b[38;5;28;01melse\u001b[39;00m (num,)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\prng.py:569\u001b[0m, in \u001b[0;36mrandom_split\u001b[1;34m(keys, shape)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split\u001b[39m(keys, shape: Shape):\n\u001b[1;32m--> 569\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_split_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\core.py:416\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    414\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    415\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[1;32m--> 416\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\core.py:420\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m    419\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[1;32m--> 420\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\core.py:921\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 921\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\prng.py:581\u001b[0m, in \u001b[0;36mrandom_split_impl\u001b[1;34m(keys, shape)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;129m@random_split_p\u001b[39m\u001b[38;5;241m.\u001b[39mdef_impl\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split_impl\u001b[39m(keys, \u001b[38;5;241m*\u001b[39m, shape):\n\u001b[1;32m--> 581\u001b[0m   base_arr \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_split_impl_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PRNGKeyArray(keys\u001b[38;5;241m.\u001b[39m_impl, base_arr)\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\prng.py:587\u001b[0m, in \u001b[0;36mrandom_split_impl_base\u001b[1;34m(impl, base_arr, keys_ndim, shape)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split_impl_base\u001b[39m(impl, base_arr, keys_ndim, \u001b[38;5;241m*\u001b[39m, shape):\n\u001b[0;32m    586\u001b[0m   split \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(keys_ndim, \u001b[38;5;28;01mlambda\u001b[39;00m k: impl\u001b[38;5;241m.\u001b[39msplit(k, shape))\n\u001b[1;32m--> 587\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_arr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\prng.py:586\u001b[0m, in \u001b[0;36mrandom_split_impl_base.<locals>.<lambda>\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split_impl_base\u001b[39m(impl, base_arr, keys_ndim, \u001b[38;5;241m*\u001b[39m, shape):\n\u001b[1;32m--> 586\u001b[0m   split \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(keys_ndim, \u001b[38;5;28;01mlambda\u001b[39;00m k: \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    587\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m split(base_arr)\n",
      "File \u001b[1;32mc:\\Users\\Manju\\Documents\\python\\envs\\myenv\\lib\\site-packages\\jax\\_src\\prng.py:1101\u001b[0m, in \u001b[0;36mthreefry_split\u001b[1;34m(key, shape)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreefry_split\u001b[39m(key: typing\u001b[38;5;241m.\u001b[39mArray, shape: Shape) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mArray:\n\u001b[0;32m   1100\u001b[0m   shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(unsafe_map(core\u001b[38;5;241m.\u001b[39mconcrete_dim_or_error, shape))\n\u001b[1;32m-> 1101\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_threefry_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "from spifol_archs import FNOBlock2D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # saving the parameter\n",
    "def save_model(model, filename):\n",
    "    # Save model parameters, architecture and optimizer state\n",
    "    save_dict = {\n",
    "        'arch': model.arch,\n",
    "        'N': model.N,\n",
    "        'lr': model.lr,\n",
    "        'eps': model.eps,\n",
    "        'pp2': model.pp2,\n",
    "        'qq2': model.qq2,\n",
    "        'dt': model.dt,\n",
    "        'L': model.L,\n",
    "        'h': model.h,\n",
    "        'x': model.x,\n",
    "        'y': model.y,\n",
    "        'params': jax.device_get(model.get_params(model.opt_state)),\n",
    "        'train_losses': model.train_losses,\n",
    "        'test_losses': model.test_losses,\n",
    "        'opt_state': jax.device_get(model.opt_state),  # Save optimizer state too\n",
    "    }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N//self.batch_size\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # Initialize the network based on architecture type\n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(random.PRNGKey(1234), (-1, N, N, 1))\n",
    "            \n",
    "        elif arch == 'MLP':\n",
    "            self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        elif arch == 'modified_MLP':\n",
    "            self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "            )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        if self.arch == 'FNO':\n",
    "            print(\"Shape of uk before reshaping (inside operator net):\", uk.shape)  # Debug\n",
    "\n",
    "            \n",
    "            input_FNO = uk.reshape(-1, self.N, self.N, 1)  # Reshape for FNO\n",
    "\n",
    "            print(\"Shape of uk after reshaping (inside operator net):\", uk.shape)  # Debug\n",
    "\n",
    "    \n",
    "            O = self.N_apply(params, input_FNO)  # Apply the FNO network \n",
    "            print(\"Shape of uk after N-apply  (inside operator net):\", O.shape)  # Debug\n",
    "\n",
    "            O = O.reshape(self.N, self.N, 1)  # Reshape output\n",
    "\n",
    "            print(\"Shape of uk after N-apply and reshape (inside operator net):\", O.shape)  # Debug\n",
    "\n",
    "            return O\n",
    "        elif self.arch == 'MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)  # Directly apply the network\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "            return O\n",
    "        elif self.arch == 'modified_MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "      \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def allen_cahn_equation(self, uk):\n",
    "        \n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))\n",
    "        # print(\"Denominator shape:\", denominator.shape)\n",
    "\n",
    "        # Expand the denominator to match the shape of s_hat (28, 28, 1)\n",
    "        denominator = denominator[..., None]  # Add a third dimension to make the shape (28, 28, 1)\n",
    "        # print(\"Denominator shape after expansion:\", denominator.shape)\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        # print(\"Shape of s_hat (after fft2):\", s_hat.shape)\n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "        # print(\"Shape of v_hat (after division):\", v_hat.shape)\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        # print(\"Shape of uk (after ifft2):\", uk.shape)\n",
    "\n",
    "        uk = uk.reshape(self.N, self.N, 1)  # Reshaping to (N, N, 1)\n",
    "        # print(\"Shape of uk after reshaping:\", uk.shape)\n",
    "\n",
    "        # Return the real part\n",
    "        return jnp.real(uk)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, batch_input, batch_label):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        print(f'batch input size inside loss_single is {batch_label.shape}') \n",
    "        print(f'batch input size inside loss_single is {batch_label.shape}') \n",
    "\n",
    "        u_nn = self.operator_net(params, batch_input) # predicted or next value of the initial condition\n",
    "        print(f'batch input size inside loss_single (network prediction) is {u_nn.shape}') \n",
    "\n",
    "        u_nn = u_nn.reshape(self.N, self.N, 1)     \n",
    "       \n",
    "        datadriven_loss = jnp.mean((batch_label - u_nn) ** 2)\n",
    "        return datadriven_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch_input, batch_label):\n",
    "        # print(f'vmap agadi :{batch_input.shape, batch_label.shape}')\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0, 0))(params, batch_input, batch_label)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch_input, batch_label):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, batch_input, batch_label)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, datatrain_input, datatrain_label,  datatest_input, datatest_label, nIter=10000):\n",
    "        datainput_train_iter = iter(datatrain_input)\n",
    "        datatrain_label_iter = iter(datatrain_label)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch_input_train = next(datainput_train_iter)\n",
    "            batch_label_train= next(datatrain_label_iter)\n",
    "            batch_input_train = jnp.array(batch_input_train)\n",
    "            batch_label_train = jnp.array(batch_label_train)\n",
    "            \n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch_input_train, batch_label_train)\n",
    "\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss_train = self.loss_batches(params, batch_input_train, batch_label_train)\n",
    "                self.train_losses.append(loss_train)\n",
    "\n",
    "\n",
    "                # for testing\n",
    "                test_loss = []\n",
    "                for _ in range(len(datatest_input)):\n",
    "                    datatest_input_iter= iter(datatest_input)\n",
    "                    datatest_label_iter= iter(datatest_label)\n",
    "                    batch_input_test = next(datatest_input_iter)\n",
    "                    batch_label_test= next(datatest_label_iter)\n",
    "                    batch_input_test = jnp.array(batch_input_test)\n",
    "                    batch_label_test = jnp.array(batch_label_test)\n",
    "\n",
    "\n",
    "                    test_batch_loss = self.loss_batches(params, batch_input_test, batch_label_test)\n",
    "                    test_loss.append(test_batch_loss)\n",
    "                \n",
    "                mean_test_loss = jnp.mean(jnp.array(test_loss))\n",
    "                    \n",
    "                self.test_losses.append(mean_test_loss)\n",
    "            pbar.set_postfix({'train Loss': loss_train, 'test loss': mean_test_loss})\n",
    "\n",
    "\n",
    "    def pred(self, data_test, data_label):\n",
    "        # uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            # uk = self.allen_cahn_equation(item)\n",
    "            # cahn = eps**2\n",
    "            # uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # # Compute denominator in Fourier space\n",
    "            # denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # # Perform FFT calculations\n",
    "            # s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            # v_hat = s_hat / denominator  \n",
    "            # uk_ac = jfft.ifft2(v_hat)  \n",
    "            # uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            # uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        # uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(data_label, (data_label.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_nnetwork\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.title(\"data_driven_training\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.05 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.0001 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "# print(f'pp2 shape:{pp2.shape}')\n",
    "# print(f'qq2 shape:{qq2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "# Define FNO layers\n",
    "fno_layers = [\n",
    "   Dense(64),\n",
    "   Permute(\"ijkl->iljk\"),\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,  # activation can be changed here\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,\n",
    "   FNOBlock2D(15),\n",
    "   Permute(\"ijkl->iklj\"),\n",
    "   Dense(128),\n",
    "   Gelu,\n",
    "   Dense(1),\n",
    "]\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 5\n",
    "\n",
    "data = np.load(\"../data_driven/data/driven_data_28x28_1ksample_41timestep_every1kiter.npy\")\n",
    "# pairing the dataset ex 0_timesteps -> 4k_timestep, 4k_timestep -> 8k_timesteps so on.\n",
    "data_input = data[:, :-1, :, :]\n",
    "data_label = data[:, 1:, :, :]\n",
    "# print(f'dataset ko shape:{data_input.shape, data_label.shape}')\n",
    "\n",
    "# data_plot_input = data_input[610][0]# data_plot_label = data_label[610][9]\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 4))  # 11 time steps\n",
    "# print(axes)\n",
    "# axes[0].imshow(data_plot_input)\n",
    "# axes[1].imshow(data_plot_label)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Generate the data trainig samples\n",
    "data_input_reshape = data_input.reshape(-1, N, N, 1)\n",
    "data_label_reshape = data_label.reshape(-1, N, N, 1) # label is the gt here \n",
    "# print(f'dataset ko shape after reshape:{data_input_reshape.shape, data_label_reshape.shape}')\n",
    "\n",
    "# Split the dataset\n",
    "train_input, test_input, train_label, test_label = train_test_split(\n",
    "    data_input_reshape, data_label_reshape, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input.shape}\")\n",
    "# print(f\"Test Input Shape: {test_input.shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label.shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label.shape}\")\n",
    "\n",
    "train_input_batch = DataGenerator(train_input, batch_size=20)\n",
    "test_input_batch = DataGenerator(test_input, batch_size=20)\n",
    "train_label_batch = DataGenerator(train_label, batch_size=20)\n",
    "test_label_batch = DataGenerator(test_label, batch_size=20)\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input_batch[0].shape}\")\n",
    "# print(f\"Test Input Shape: {test_input_batch[0].shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label_batch[0].shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label_batch[0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  # Specify the directory where y want to save the data\n",
    "# save_dir = './data_driven//'\n",
    "\n",
    "#      # Ensure the directory exists, create it if not\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "NN_model.train(train_input_batch, train_label_batch,  test_input_batch, test_label_batch, nIter = epochs)\n",
    "\n",
    "# Now call loss function\n",
    "# loss = NN_model.loss_batches(params, batch_input, batch_label)\n",
    "r2, mse, l2_rel, u_pred = NN_model.pred(test_input, test_label)\n",
    "print(f'r2:{r2},mse : {mse}, l2_rel : {l2_rel}')\n",
    "\n",
    "# NN_model.plot_losses(f'data_driven/data_driven_plot/data_driven_every2ktimesiter_training_log_iter_{epochs}.png')\n",
    "\n",
    "\n",
    "# save_model(NN_model, f'data_driven/models/data_driven_pairstrategy_model_every2ktimestep_{epochs}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
