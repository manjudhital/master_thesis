{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.random as random\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.fft as jfft\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "from numpy import sqrt\n",
    "from numpy import round\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import contour\n",
    "from jax.numpy.fft import fft2, ifft2\n",
    "from jax.numpy.fft import fftn, ifftn\n",
    "from numpy import real\n",
    "from jax.example_libraries.stax import serial, Gelu\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "import os \n",
    "# @partial(jit, static_argnums=(0,))\n",
    "def allen_cahn_equation(uk, pp2, qq2, dt, eps, Nt):\n",
    "\n",
    "    cahn = eps**2\n",
    "    samples_timesteps = []\n",
    "\n",
    "    for iter in range(0, Nt+1):\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + dt * (2 + cahn * (pp2 + qq2))\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk)) \n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        uk = jnp.real(uk)\n",
    "        if iter % 1000 == 0:\n",
    "            samples_timesteps.append(uk)\n",
    "\n",
    "            # this is to see the how many samples are completed\n",
    "            print(f'sample {iter} completed')\n",
    "\n",
    "        # Return the real part\n",
    "    return jnp.array(samples_timesteps)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# difinning the no of grid points in x, y and z\n",
    "Nx = 28 # number of grid points in x be positive even integer number\n",
    "Ny = 28 # number of grid points in y be positive even integer number\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameters of the Allen-Cahn equation in 2d\n",
    "Lx = 2.0 * jnp.pi #length of the domain in x\n",
    "Ly = 2.0 * jnp.pi #length of the domain in y\n",
    "hx = Lx / Nx #spatial step size in coordinate x\n",
    "hy = Ly / Ny #spatial step size in coordinate y\n",
    "dt = 0.0001 #time step size\n",
    "T = 4 #final time\n",
    "Nt = int(jnp.round(T/dt)) #number of time steps\n",
    "ns = Nt / 10 #number of snapshots\n",
    "\n",
    "# Define the grid points in x and y direction\n",
    "def x_gridpoint(Nx, Lx, hx):\n",
    "    x = jnp.linspace(-0.5*Lx+hx,0.5*Lx,Nx)\n",
    "    return x\n",
    "x = x_gridpoint(Nx, Lx, hx) #number of grid points in x direction and step size and limitation on x  axis\n",
    "def y_gridpoint(Ny, Ly, hy):\n",
    "    y = jnp.linspace(-0.5*Ly+hy,0.5*Ly,Ny)\n",
    "    return y\n",
    "y = y_gridpoint(Ny, Ly, hy) #number of grid points in y direction and step size and limitation on y  axis \n",
    "\n",
    "# creating meshgrid in x and y direction\n",
    "xx,yy = jnp.meshgrid(x,y) #creating meshgrid in x and y direction \n",
    "\n",
    "epsillon = 0.5 #small parameter # interface thickness in the Allen-Cahn equation \n",
    "cahn = epsillon**2 #cahn number  \n",
    "\n",
    "# theta = jnp.arctan2(yy, xx)\n",
    "#   # or another appropriate value\n",
    "# uk = jnp.tanh((1.7 + 1.2 * np.cos(6 * theta)) - jnp.sqrt(xx**2 + yy**2) / (jnp.sqrt(2) * epsillon))\n",
    "data = np.load('data_generation_checking/phasefield2d_data_28x28_10k.npy')\n",
    "\n",
    "# Select 1,000 random samples\n",
    "key = jax.random.PRNGKey(0)  # Random seed for reproducibility\n",
    "idx = jax.random.choice(key, data.shape[0], shape=(1000,), replace=False)  # Random 1k indices\n",
    "input_samples = data[idx]  # Shape: (1000, Nx, Ny)\n",
    "# print(f'uk ko shape:{uk.shape}')\n",
    "\n",
    "\n",
    "# defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / Lx * jnp.arange(0, Nx//2), 2 * jnp.pi / Lx * jnp.arange(-Nx//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / Ly * jnp.arange(0, Ny//2), 2 * jnp.pi / Ly * jnp.arange(-Ny//2 , 0)])\n",
    "\n",
    "\n",
    "# square of wavenumber in x and y direction\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "\n",
    "# creating meshgrid in x and y direction for square of wavenumber\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "\n",
    "\n",
    "input_samples= input_samples.reshape(-1, Nx , Ny)\n",
    "\n",
    "samples = []\n",
    "\n",
    "# this for to see the how many samples are printed \n",
    "for i, uk in enumerate (input_samples):\n",
    "    print(f'sample {i} started')\n",
    "\n",
    "# for uk in input_samples:\n",
    "   \n",
    "    ac_input = allen_cahn_equation(uk, pp2, qq2, dt, epsillon, Nt)\n",
    "# print(f'shape of ac_input:{ac_input.shape}')\n",
    "    samples.append(ac_input)\n",
    "samples = jnp.array(samples)\n",
    "# print(f'samples ko shape:{samples.shape}')\n",
    "      \n",
    "\n",
    "# Specify the directory where y want to save the data\n",
    "save_dir = './data_driven/data'\n",
    "\n",
    "    # Ensure the directory exists, create it if not\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#saving the data \n",
    "# Save the training and testing data\n",
    "# print(\"Saving training data to u_train.npy...\")\n",
    "np.save(os.path.join(save_dir, \"driven_data_28x28_1k_input_samples_20timestep_every2kiter.npy\"), np.array(input_samples))\n",
    "np.save(os.path.join(save_dir, \"driven_data_28x28_1ksample_20timestep_every2kiter.npy\"), np.array(samples))\n",
    "\n",
    "# loaded the generated data\n",
    "loaded_input_samples = np.load(os.path.join(save_dir, \"driven_data_28x28_1k_input_samples_20timestep_every2kiter.npy\"))\n",
    "print(f'loaded_input_samples ko shape:{loaded_input_samples.shape}')\n",
    "\n",
    "loaded_samples = np.load(os.path.join(save_dir, \"driven_data_28x28_1ksample_20timestep_every2kiter.npy\"))\n",
    "print(f'loaded_samples ko shape:{loaded_samples.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visulized the genrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_dir = './data_driven'\n",
    "\n",
    "# Choose a sample index\n",
    "sample_idx = 13\n",
    "\n",
    "# Define the directory\n",
    "plot_save_dir = os.path.join(save_dir,  \"data_driven_plot\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Plot input data\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(loaded_input_samples[sample_idx], cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(f'Input Sample_ 2k_intr_{sample_idx}')\n",
    "plt.savefig(os.path.join(plot_save_dir, f'input_sample_2k_iter_{sample_idx}.png')) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Choose a sample index\n",
    "sample_idx = 13\n",
    "\n",
    "fig, axes = plt.subplots(1, 11, figsize=(20, 4))  # 11 time steps\n",
    "\n",
    "for t in range(11):\n",
    "    axes[t].imshow(loaded_samples[sample_idx, t], cmap='viridis')\n",
    "    axes[t].set_title(f'Time {t * 2000}')\n",
    "    axes[t].axis('off')\n",
    "\n",
    "plt.suptitle(f'Evolution of Sample_2k_iter_{sample_idx}')\n",
    "plt.savefig(os.path.join(plot_save_dir, f'Evolution_sample_2k_iter_{sample_idx}.png'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(loaded_samples[sample_idx, 0], cmap='viridis')\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(loaded_samples[sample_idx, frame])\n",
    "    ax.set_title(f'Time Step: {frame * 2000}')\n",
    "    return im,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=11, interval=500)\n",
    "\n",
    "# ani.save(evolution_animation.gif', writer='pillow')  # Save as GIF\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "from spifol_archs import FNOBlock2D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # saving the parameter\n",
    "def save_model(model, filename):\n",
    "    # Save model parameters, architecture and optimizer state\n",
    "    save_dict = {\n",
    "        'arch': model.arch,\n",
    "        'N': model.N,\n",
    "        'lr': model.lr,\n",
    "        'eps': model.eps,\n",
    "        'pp2': model.pp2,\n",
    "        'qq2': model.qq2,\n",
    "        'dt': model.dt,\n",
    "        'L': model.L,\n",
    "        'h': model.h,\n",
    "        'x': model.x,\n",
    "        'y': model.y,\n",
    "        'params': jax.device_get(model.get_params(model.opt_state)),\n",
    "        'train_losses': model.train_losses,\n",
    "        'test_losses': model.test_losses,\n",
    "        'opt_state': jax.device_get(model.opt_state),  # Save optimizer state too\n",
    "    }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N//self.batch_size\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # Initialize the network based on architecture type\n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(random.PRNGKey(1234), (-1, N, N, 1))\n",
    "            \n",
    "        elif arch == 'MLP':\n",
    "            self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        elif arch == 'modified_MLP':\n",
    "            self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "            )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        if self.arch == 'FNO':\n",
    "            \n",
    "            input_FNO = uk.reshape(-1, self.N, self.N, 1)  # Reshape for FNO\n",
    "    \n",
    "            O = self.N_apply(params, input_FNO)  # Apply the FNO network \n",
    "            O = O.reshape(self.N, self.N, 1)  # Reshape output\n",
    "            return O\n",
    "        elif self.arch == 'MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)  # Directly apply the network\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "            return O\n",
    "        elif self.arch == 'modified_MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "      \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def allen_cahn_equation(self, uk):\n",
    "        \n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))\n",
    "        # print(\"Denominator shape:\", denominator.shape)\n",
    "\n",
    "        # Expand the denominator to match the shape of s_hat (28, 28, 1)\n",
    "        denominator = denominator[..., None]  # Add a third dimension to make the shape (28, 28, 1)\n",
    "        # print(\"Denominator shape after expansion:\", denominator.shape)\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        # print(\"Shape of s_hat (after fft2):\", s_hat.shape)\n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "        # print(\"Shape of v_hat (after division):\", v_hat.shape)\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        # print(\"Shape of uk (after ifft2):\", uk.shape)\n",
    "\n",
    "        uk = uk.reshape(self.N, self.N, 1)  # Reshaping to (N, N, 1)\n",
    "        # print(\"Shape of uk after reshaping:\", uk.shape)\n",
    "\n",
    "        # Return the real part\n",
    "        return jnp.real(uk)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, batch_input, batch_label):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        \n",
    "        u_nn = self.operator_net(params, batch_input) # predicted or next value of the initial condition\n",
    "        u_nn = u_nn.reshape(self.N, self.N, 1)     \n",
    "       \n",
    "        datadriven_loss = jnp.mean((batch_label - u_nn) ** 2)\n",
    "        return datadriven_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch_input, batch_label):\n",
    "        # print(f'vmap agadi :{batch_input.shape, batch_label.shape}')\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0, 0))(params, batch_input, batch_label)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch_input, batch_label):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, batch_input, batch_label)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, datatrain_input, datatrain_label,  datatest_input, datatest_label, nIter=10000):\n",
    "        datainput_train_iter = iter(datatrain_input)\n",
    "        datatrain_label_iter = iter(datatrain_label)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch_input_train = next(datainput_train_iter)\n",
    "            batch_label_train= next(datatrain_label_iter)\n",
    "            batch_input_train = jnp.array(batch_input_train)\n",
    "            batch_label_train = jnp.array(batch_label_train)\n",
    "            \n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch_input_train, batch_label_train)\n",
    "\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss_train = self.loss_batches(params, batch_input_train, batch_label_train)\n",
    "                self.train_losses.append(loss_train)\n",
    "\n",
    "\n",
    "                # for testing\n",
    "                test_loss = []\n",
    "                for _ in range(len(datatest_input)):\n",
    "                    datatest_input_iter= iter(datatest_input)\n",
    "                    datatest_label_iter= iter(datatest_label)\n",
    "                    batch_input_test = next(datatest_input_iter)\n",
    "                    batch_label_test= next(datatest_label_iter)\n",
    "                    batch_input_test = jnp.array(batch_input_test)\n",
    "                    batch_label_test = jnp.array(batch_label_test)\n",
    "\n",
    "\n",
    "                    test_batch_loss = self.loss_batches(params, batch_input_test, batch_label_test)\n",
    "                    test_loss.append(test_batch_loss)\n",
    "                \n",
    "                mean_test_loss = jnp.mean(jnp.array(test_loss))\n",
    "                    \n",
    "                self.test_losses.append(mean_test_loss)\n",
    "            pbar.set_postfix({'train Loss': loss_train, 'test loss': mean_test_loss})\n",
    "\n",
    "\n",
    "    def pred(self, data_test, data_label):\n",
    "        # uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            # uk = self.allen_cahn_equation(item)\n",
    "            # cahn = eps**2\n",
    "            # uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # # Compute denominator in Fourier space\n",
    "            # denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # # Perform FFT calculations\n",
    "            # s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            # v_hat = s_hat / denominator  \n",
    "            # uk_ac = jfft.ifft2(v_hat)  \n",
    "            # uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            # uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        # uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(data_label, (data_label.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_nnetwork\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.title(\"data_driven_training\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.05 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.0001 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "# print(f'pp2 shape:{pp2.shape}')\n",
    "# print(f'qq2 shape:{qq2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "# Define FNO layers\n",
    "fno_layers = [\n",
    "   Dense(64),\n",
    "   Permute(\"ijkl->iljk\"),\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,  # activation can be changed here\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,\n",
    "   FNOBlock2D(15),\n",
    "   Permute(\"ijkl->iklj\"),\n",
    "   Dense(128),\n",
    "   Gelu,\n",
    "   Dense(1),\n",
    "]\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 10000\n",
    "\n",
    "data = np.load('data_driven/data/driven_data_28x28_1ksample_20timestep_every2kiter.npy') \n",
    "# pairing the dataset ex 0_timesteps -> 4k_timestep, 4k_timestep -> 8k_timesteps so on.\n",
    "data_input = data[:, :-1, :, :]\n",
    "data_label = data[:, 1:, :, :]\n",
    "# print(f'dataset ko shape:{data_input.shape, data_label.shape}')\n",
    "\n",
    "# data_plot_input = data_input[610][0]# data_plot_label = data_label[610][9]\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 4))  # 11 time steps\n",
    "# print(axes)\n",
    "# axes[0].imshow(data_plot_input)\n",
    "# axes[1].imshow(data_plot_label)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Generate the data trainig samples\n",
    "data_input_reshape = data_input.reshape(-1, N, N, 1)\n",
    "data_label_reshape = data_label.reshape(-1, N, N, 1) # label is the gt here \n",
    "# print(f'dataset ko shape after reshape:{data_input_reshape.shape, data_label_reshape.shape}')\n",
    "\n",
    "# Split the dataset\n",
    "train_input, test_input, train_label, test_label = train_test_split(\n",
    "    data_input_reshape, data_label_reshape, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input.shape}\")\n",
    "# print(f\"Test Input Shape: {test_input.shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label.shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label.shape}\")\n",
    "\n",
    "train_input_batch = DataGenerator(train_input, batch_size=20)\n",
    "test_input_batch = DataGenerator(test_input, batch_size=20)\n",
    "train_label_batch = DataGenerator(train_label, batch_size=20)\n",
    "test_label_batch = DataGenerator(test_label, batch_size=20)\n",
    "\n",
    "# Print shapes to verify\n",
    "# print(f\"Train Input Shape: {train_input_batch[0].shape}\")\n",
    "# print(f\"Test Input Shape: {test_input_batch[0].shape}\")\n",
    "# print(f\"Train Prediction Shape: {train_label_batch[0].shape}\")\n",
    "# print(f\"Test Prediction Shape: {test_label_batch[0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  # Specify the directory where y want to save the data\n",
    "# save_dir = './data_driven//'\n",
    "\n",
    "#      # Ensure the directory exists, create it if not\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "NN_model.train(train_input_batch, train_label_batch,  test_input_batch, test_label_batch, nIter = epochs)\n",
    "\n",
    "# Now call loss function\n",
    "# loss = NN_model.loss_batches(params, batch_input, batch_label)\n",
    "r2, mse, l2_rel, u_pred = NN_model.pred(test_input, test_label)\n",
    "print(f'r2:{r2},mse : {mse}, l2_rel : {l2_rel}')\n",
    "\n",
    "NN_model.plot_losses(f'data_driven/data_driven_plot/data_driven_every2ktimesiter_training_log_iter_{epochs}.png')\n",
    "\n",
    "\n",
    "save_model(NN_model, f'data_driven/models/data_driven_pairstrategy_model_every2ktimestep_{epochs}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule\n",
    "# from jax.scipy.fftpack import fftn, ifftn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import no_grad\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.example_libraries.optimizers import exponential_decay\n",
    "import jax.numpy.fft as jfft\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial, glorot_normal\n",
    "from spifol_archs import FNOBlock2D, Permute, complex_adam, MLP, modified_MLP\n",
    "from jax import vmap\n",
    "from torch.utils import data\n",
    "from jax import lax\n",
    "from jax import debug\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from matplotlib.cm import tab20\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle\n",
    "import os\n",
    "from numpy import sqrt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fft2(x):\n",
    "   \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return fftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "def ifft2(x):\n",
    "   \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "   return ifftn(x, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def normalize(data):\n",
    "    min_val = jnp.min(data, axis=(0, 1))\n",
    "    max_val = jnp.max(data, axis=(0, 1))\n",
    "    range_val = max_val - min_val\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Avoid division by zero\n",
    "    normalized_data = 2 * (data - min_val) / range_val - 1\n",
    "    return normalized_data, min_val, range_val \n",
    "\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, min_val, range_val):\n",
    "    range_val = jnp.where(range_val == 0, 1, range_val)  # Ensure no division by zero\n",
    "    data = ((normalized_data + 1) * range_val) / 2 + min_val\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SPiFOL:\n",
    "    def __init__(self, L, x, y, h, eps, pp2, qq2, dt,  N, fno_layers, mlp_layers,lr, arch):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        # self.norm_par = norm_par\n",
    "        self.eps = eps\n",
    "        self.pp2 = pp2\n",
    "        self.qq2 = qq2\n",
    "        self.dt = dt\n",
    "        self.L = L\n",
    "        self.h = h\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        # Initialize the network based on architecture type\n",
    "        if arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(random.PRNGKey(1234), (-1, N, N, 1))\n",
    "            \n",
    "        elif arch == 'MLP':\n",
    "            self.N_init, self.N_apply = MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "            \n",
    "        elif arch == 'modified_MLP':\n",
    "            self.N_init, self.N_apply = modified_MLP(mlp_layers)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9)\n",
    "            )\n",
    "\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        \n",
    "\n",
    "\n",
    "        # Logging losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []  # Initialize list to track test losses\n",
    "\n",
    "\n",
    "          # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "       \n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, uk):\n",
    "        if self.arch == 'FNO':\n",
    "            \n",
    "            input_FNO = uk.reshape(-1, self.N, self.N, 1)  # Reshape for FNO\n",
    "    \n",
    "            O = self.N_apply(params, input_FNO)  # Apply the FNO network \n",
    "            O = O.reshape(self.N, self.N, 1)  # Reshape output\n",
    "            return O\n",
    "        elif self.arch == 'MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)  # Directly apply the network\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])  # Reshape output to match strain components\n",
    "            return O\n",
    "        elif self.arch == 'modified_MLP':\n",
    "            uk = uk.flatten()\n",
    "            O = self.N_apply(params, uk)\n",
    "            O = O.reshape(uk.shape[0], self.N, self.N, uk.shape[3])\n",
    "            return O\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "      \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def allen_cahn_equation(self, uk):\n",
    "        \n",
    "        cahn = eps**2\n",
    "        uk = jnp.real(uk)\n",
    "\n",
    "        # Compute denominator in Fourier space\n",
    "        denominator = cahn + self.dt * (2 + cahn * (self.pp2 + self.qq2))\n",
    "        # print(\"Denominator shape:\", denominator.shape)\n",
    "\n",
    "        # Expand the denominator to match the shape of s_hat (28, 28, 1)\n",
    "        denominator = denominator[..., None]  # Add a third dimension to make the shape (28, 28, 1)\n",
    "        print(\"Denominator shape after expansion:\", denominator.shape)\n",
    "\n",
    "        # Perform FFT calculations\n",
    "        s_hat = jfft.fft2(cahn * uk - self.dt * (uk**3 - 3 * uk)) \n",
    "        # print(\"Shape of s_hat (after fft2):\", s_hat.shape)\n",
    "\n",
    "        v_hat = s_hat / denominator  # Now shapes should match\n",
    "        # print(\"Shape of v_hat (after division):\", v_hat.shape)\n",
    "\n",
    "        uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "        # print(\"Shape of uk (after ifft2):\", uk.shape)\n",
    "\n",
    "        uk = uk.reshape(self.N, self.N, 1)  # Reshaping to (N, N, 1)\n",
    "        # print(\"Shape of uk after reshaping:\", uk.shape)\n",
    "\n",
    "        # Return the real part\n",
    "        return jnp.real(uk)  # Return only the real part\n",
    "\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, uk):\n",
    "        # uk is the input data and u_nn is the next uK+1 data of neural network and u_ac is also next u_ac_k+1 data\n",
    "        \n",
    "        u_nn = self.operator_net(params, uk) # predicted or next value of the initial condition\n",
    "        u_nn = u_nn.reshape(self.N, self.N, 1)     \n",
    "        u_ac = self.allen_cahn_equation(uk)\n",
    "        datadriven_loss = jnp.mean((u_ac - u_nn) ** 2)\n",
    "        return datadriven_loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch):\n",
    "       \n",
    "        batch_loss = vmap(self.loss_single, (None, 0))(params, batch)\n",
    "        batch_loss  = jnp.mean(batch_loss)\n",
    "        return batch_loss\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, uk):\n",
    "        params = self.get_params(opt_state)\n",
    "        grads = grad(self.loss_batches)(params, uk)\n",
    "        return self.opt_update(i, grads, opt_state)\n",
    "\n",
    "\n",
    "   # Update the train method of tum_epochshe SPiFOL class\n",
    "    def train(self, dataset, data_test, nIter=10000):\n",
    "        data = iter(dataset)\n",
    "        pbar = trange(nIter)  # Progress bar for total iterations\n",
    "\n",
    "\n",
    "        for it in pbar:\n",
    "            batch = next(data)\n",
    "            batch = jnp.array(batch)\n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
    "\n",
    "            if it % 1 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss = self.loss_batches(params, batch)\n",
    "                loss_test = self.loss_batches(params, data_test)\n",
    "                self.train_losses.append(loss)\n",
    "                self.test_losses.append(loss_test)\n",
    "                pbar.set_postfix({'train Loss': loss, 'test loss': loss_test})\n",
    "\n",
    "\n",
    "    def pred(self, data_test):\n",
    "        uk_solver_list = []\n",
    "        uk_nnetwork_list = []\n",
    "        \n",
    "\n",
    "        for item in data_test:\n",
    "\n",
    "            uk = self.allen_cahn_equation(item)\n",
    "            # cahn = eps**2\n",
    "            # uk = jnp.real(item)\n",
    "           \n",
    "\n",
    "            # # Compute denominator in Fourier space\n",
    "            # denominator = cahn + dt * (2 + cahn * (pp2 + qq2)) \n",
    "            \n",
    "            # # Perform FFT calculations\n",
    "            # s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk))  \n",
    "            # v_hat = s_hat / denominator  \n",
    "            # uk_ac = jfft.ifft2(v_hat)  \n",
    "            uk_ac = uk.reshape(self.N, self.N, 1)\n",
    "            uk_solver_list.append(uk_ac)\n",
    "\n",
    "            params = self.get_params(self.opt_state)\n",
    "    \n",
    "            uk_nnetwork = self.operator_net(params, item)\n",
    "            uk_nnetwork_list.append(uk_nnetwork)\n",
    "        uk_solver = jnp.array(uk_solver_list)\n",
    "        uk_nnetwork = jnp.array(uk_nnetwork_list)\n",
    "\n",
    "        #  flatten \n",
    "        u_pred = jnp.reshape(uk_nnetwork, (uk_nnetwork.shape[0], -1 ))  \n",
    "        u_true = jnp.reshape(uk_solver, (uk_solver.shape[0], -1))  \n",
    "        \n",
    "        # Compute R² Score\n",
    "        r2 = r2_score(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(jnp.array(u_true), jnp.array(u_pred))  \n",
    "\n",
    "        # Compute L₂ Relative Error (normalized error)\n",
    "        l2_rel = jnp.linalg.norm(u_true - u_pred) / jnp.linalg.norm(u_true)  # L2 error\n",
    "\n",
    "        \n",
    "        \n",
    "        return r2, mse, l2_rel, uk_solver, uk_nnetwork\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "            # Convert loss array and jax numpy array for plotting\n",
    "            total_train_loss = jnp.asarray(self.train_losses)\n",
    "            total_test_loss = jnp.asarray(self.test_losses)\n",
    "            \n",
    "            \n",
    "            #print(total_loss)\n",
    "            color = tab20.colors\n",
    "            x_axis = jnp.arange(1, total_train_loss.size + 1, 1) # x_axis: Epoch numbers from 1 to 100\n",
    "\n",
    "            #print(x_axis)\n",
    "            # Create plot\n",
    "            plt.figure(constrained_layout=True)\n",
    "            ax = plt.subplot(111)\n",
    "\n",
    "            plt.semilogy(x_axis, total_train_loss, label=\"Train\", c=color[0])\n",
    "            plt.semilogy(x_axis, total_test_loss, label=\"Test\", c=color[6])\n",
    "            #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "            #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "            box = ax.get_position()\n",
    "            ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "            plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = 28 # no. of grid points\n",
    "eps = 0.5 # epsillon \n",
    "lr = 0.001 # learning rate\n",
    "dt = 0.0001 # time step or time increment\n",
    "L = 2 * jnp.pi # length of domian\n",
    "h = L/N # spacing between grid or length of grid\n",
    "T = 4\n",
    "Nt = int(jnp.round(T/dt))\n",
    "x = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "y = jnp.linspace(-0.5 * L + h, 0.5 * L, N)\n",
    "xx, yy = jnp.meshgrid(x, y)\n",
    "\n",
    "\n",
    " # number of epochs for training\n",
    "\n",
    "\n",
    " # defining the wavenumber in x and y direction , which is in fourier space\n",
    "p = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2  , 0)]) # wavenumber in x direction\n",
    "q = jnp.concatenate([2 * jnp.pi / L * jnp.arange(0, N//2), 2 * jnp.pi / L * jnp.arange(-N//2 , 0)])\n",
    "p2 = p**2 # square of wavenumber in x direction\n",
    "q2 = q**2 # square of wavenumber in y direction\n",
    "pp2, qq2 = jnp.meshgrid(p2, q2)\n",
    "# print(f'pp2 shape:{pp2.shape}')\n",
    "# print(f'qq2 shape:{qq2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_layers = [16384, 32, 32, 16384]\n",
    "\n",
    "\n",
    "# Define FNO layers\n",
    "fno_layers = [\n",
    "   Dense(64),\n",
    "   Permute(\"ijkl->iljk\"),\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,  # activation can be changed here\n",
    "   FNOBlock2D(15),\n",
    "   Gelu,\n",
    "   FNOBlock2D(15),\n",
    "   Permute(\"ijkl->iklj\"),\n",
    "   Dense(128),\n",
    "   Gelu,\n",
    "   Dense(1),\n",
    "]\n",
    "\n",
    "cahn = eps**2\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    # Load the saved model data\n",
    "    with open(filename, 'rb') as f:\n",
    "        save_dict = pickle.load(f)\n",
    "\n",
    "    # Reconstruct the model based on the saved architecture type\n",
    "    arch = save_dict['arch']\n",
    "    N = save_dict['N']\n",
    "    lr = save_dict['lr']\n",
    "    eps = save_dict['eps']\n",
    "    pp2 = save_dict['pp2']\n",
    "    qq2 = save_dict['qq2']\n",
    "    dt = save_dict['dt']\n",
    "    L = save_dict['L']\n",
    "    h = save_dict['h']\n",
    "    x = save_dict['x']\n",
    "    y = save_dict['y']\n",
    "\n",
    "    # Reconstruct architecture layers (FNO, MLP, etc.)\n",
    "    if arch == 'FNO':\n",
    "        fno_layers = [\n",
    "            Dense(64),\n",
    "            Permute(\"ijkl->iljk\"),\n",
    "            FNOBlock2D(15),\n",
    "            Gelu,\n",
    "            FNOBlock2D(15),\n",
    "            Gelu,\n",
    "            FNOBlock2D(15),\n",
    "            Permute(\"ijkl->iklj\"),\n",
    "            Dense(128),\n",
    "            Gelu,\n",
    "            Dense(1),\n",
    "        ]\n",
    "        model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, [], lr, arch)\n",
    "\n",
    "    elif arch == 'MLP':\n",
    "        mlp_layers = [16384, 32, 32, 16384]\n",
    "        model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, [], mlp_layers, lr, arch)\n",
    "\n",
    "    elif arch == 'modified_MLP':\n",
    "        mlp_layers = [16384, 32, 32, 16384]\n",
    "        model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, [], mlp_layers, lr, arch)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported architecture {arch}\")\n",
    "\n",
    "    # Restore model parameters and optimizer state\n",
    "    model.params = save_dict['params']\n",
    "    model.opt_state = save_dict['opt_state']\n",
    "    model.train_losses = save_dict['train_losses']\n",
    "    model.test_losses = save_dict['test_losses']\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# theta = jnp.arctan2(yy, xx)\n",
    "# #   # or another appropriate value\n",
    "# uk = jnp.tanh((1.7 + 1.2 * np.cos(6 * theta)) - jnp.sqrt(xx**2 + yy**2) / (jnp.sqrt(2) * eps))\n",
    "# uk =jnp.tanh((2 -sqrt(xx**2 + yy**2)) / (sqrt(2) *eps))\n",
    "\n",
    "# data = jnp.real(uk)\n",
    "\n",
    "data = np.load('data_generation_checking/phasefield2d_data_28x28_10k.npy')\n",
    "\n",
    "# # normalized_data, min_val, range_val = normalize(data) \n",
    "# # Generate the data trainig samples\n",
    "# # dataset = DataGenerator(data[:9800], batch_size=20)\n",
    "# data_test = data[9800:]\n",
    "def allen_cahn_equation(uk):\n",
    "    eps = 0.5\n",
    "    cahn = eps**2\n",
    "    uk = jnp.real(uk)\n",
    "\n",
    "    # Compute denominator in Fourier space\n",
    "    denominator = cahn + dt * (2 + cahn * (pp2 + qq2))\n",
    "\n",
    "    # Perform FFT calculations\n",
    "    s_hat = jfft.fft2(cahn * uk - dt * (uk**3 - 3 * uk)) \n",
    "\n",
    "    v_hat = s_hat / denominator  # Now shapes should match\n",
    "\n",
    "\n",
    "    uk = jfft.ifft2(v_hat)  # inverse FFT\n",
    "\n",
    "    # Return the real part\n",
    "    return jnp.real(uk) \n",
    "\n",
    "# theta = jnp.arctan2(yy, xx)\n",
    "#   # or another appropriate value\n",
    "# uk = jnp.tanh((1.7 + 1.2 * np.cos(6 * theta)) - jnp.sqrt(xx**2 + yy**2) / (jnp.sqrt(2) * epsillon))\n",
    "# uk_initial = uk\n",
    "# uk_input = jnp.array(uk)\n",
    "# uk_input = uk_input.reshape(N, N, 1)\n",
    "# Directory to save images\n",
    "output_dir = \"data_driven/frames_1kstep\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "sample_idx = 13\n",
    "title = f'Evolution comparsion (Allen Cahn and data_driven_model_1kstep_15kepoch_{sample_idx}'\n",
    "\n",
    "def save_plot(data, model_data, iter_num):\n",
    "    \n",
    "    if model_data == '':\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "        im1 = axes.imshow(jnp.real(data), cmap='plasma', interpolation='bicubic')\n",
    "        axes.set_title(f\"Initial Condition\")\n",
    "        fig.colorbar(im1, ax=axes)\n",
    "        plt.savefig(f\"{output_dir}/frame_{iter_num:04d}.png\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "        im1 = axes[0].imshow(jnp.real(data), cmap='plasma', interpolation='bicubic')\n",
    "        axes[0].set_title(f\"Allen-Cahn Evolution at Iter {iter_num}\")\n",
    "        fig.colorbar(im1, ax=axes[0])\n",
    "        \n",
    "        im2 = axes[1].imshow(jnp.real(model_data), cmap='plasma', interpolation='bicubic')\n",
    "        axes[1].set_title(f\"Model Evolution at Iter {iter_num}\")\n",
    "        fig.colorbar(im2, ax=axes[1])\n",
    "        \n",
    "        plt.savefig(f\"{output_dir}/frame_{iter_num:04d}.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    # Initialize and train the model\n",
    "# NN_model = SPiFOL(L, x, y, h, eps, pp2, qq2, dt, N, fno_layers, mlp_layers, lr, arch= 'FNO')\n",
    "loaded_model = load_model('data_driven/models/data_driven_pairstrategy_model_41ktimestep_every1k_iter_15000.pkl')\n",
    "uk = data[sample_idx]\n",
    "uk_ac = uk.reshape(N, N)\n",
    "uk_initial = uk.reshape(N, N)\n",
    "save_plot(uk, '', 0)\n",
    "frames_2kstep_new = []\n",
    "params =loaded_model.get_params(loaded_model.opt_state)\n",
    "\n",
    "# Nt = 10000  # Define the number of iterations\n",
    "\n",
    "\n",
    "for iter in range(1, Nt+1):\n",
    "    \n",
    " \n",
    "    uk_ac = allen_cahn_equation(uk_ac)\n",
    "    if iter % 1000 == 0:\n",
    "       \n",
    "       \n",
    "        uk_nnetwork = loaded_model.operator_net(params, uk.reshape(N, N, 1))\n",
    "        uk_nnetwork = uk_nnetwork.reshape(N, N)\n",
    "        save_plot(uk_ac, uk_nnetwork, iter)\n",
    "        frames_2kstep_new.append(f\"{output_dir}/frame_{iter:04d}.png\")\n",
    "        uk =uk_nnetwork\n",
    "\n",
    "        # r2, mse, l2_rel, u_solver, u_pred = loaded_model.pred(uk)\n",
    "        # print(f'r2:{r2},mse : {mse}, l2_rel : {l2_rel}')\n",
    "\n",
    "       \n",
    "        # uk_ac = uk_reshape\n",
    "        \n",
    "uk_ac = uk_ac.reshape(N, N)\n",
    "# NN_model.train(dataset, data_test, nIter = epochs)\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "fig.suptitle(f\"Evolution comparsion between solver and NN every 1k time steps\", fontsize=16, fontweight='bold')\n",
    "im1 = axes[0].imshow(jnp.real(uk_initial), cmap='plasma', interpolation='bicubic')\n",
    "axes[0].set_title(f\"input condition\")\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(jnp.real(uk_ac), cmap='plasma', interpolation='bicubic')\n",
    "axes[1].set_title(f\"solver prediction_1k_timestep_{sample_idx}\")\n",
    "fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2 = axes[2].imshow(jnp.real(uk), cmap='plasma', interpolation='bicubic')\n",
    "axes[2].set_title(f\"networ prediction_1k_timesteps\")\n",
    "fig.colorbar(im1, ax=axes[2])\n",
    "plt.savefig(f\"data_driven/data_driven_plot/data_driven_evolution_comaprison_every_1kstep_15kepoch_{sample_idx}.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the result in animation form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "sample_idx = 13\n",
    "# Folder where images are saved\n",
    "folder = \"data_driven/frames_1kstep\"\n",
    "\n",
    "# Get the list of image files\n",
    "image_files = sorted(\n",
    "    [f for f in os.listdir(folder) if f.endswith(\".png\")],\n",
    "    key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    ")\n",
    "\n",
    "# Define a common size for all images (width, height)\n",
    "common_size = (1200, 600)\n",
    "\n",
    "# Read in the images and resize them using Resampling.BICUBIC\n",
    "frames_resized = []\n",
    "for filename in image_files:\n",
    "    img = Image.open(os.path.join(folder, filename)).convert(\"RGB\")  # Ensure RGB mode\n",
    "    img = img.resize(common_size, Image.Resampling.BICUBIC)  # Try BICUBIC for smoother scaling\n",
    "    frames_resized.append(np.array(img))\n",
    "\n",
    "# Create a GIF with appropriate duration\n",
    "output_dir = \"data_driven/animation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "gif_path = output_dir + f'/allen_cahn_vs_data_driven_model_evolution__15k_epoch_1ktimestep_{sample_idx}.gif'\n",
    "os.makedirs(\"animation\", exist_ok=True)\n",
    "imageio.mimsave(gif_path, frames_resized, duration=2000, format='GIF', palettesize=256)\n",
    "\n",
    "print(\"GIF created with imageio at a smoother speed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
