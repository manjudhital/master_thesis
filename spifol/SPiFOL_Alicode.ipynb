{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imoprting necessary libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from jax import random, grad, vmap, jit,jacfwd,jacrev, vjp\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.nn import relu, sigmoid, swish, elu, silu , selu\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import font_manager\n",
    "from matplotlib import font_manager\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.colors as colors\n",
    "from scipy.interpolate import interp2d\n",
    "from matplotlib.cm import tab20\n",
    "import jax.scipy as sp\n",
    "import os\n",
    "import time\n",
    "from jax.numpy.fft import fftn, ifftn, fftshift, ifftshift\n",
    "from jax.numpy.linalg import inv, det  # Make sure det is imported here\n",
    "import jaxopt\n",
    "import optax\n",
    "from jax.example_libraries.optimizers import optimizer, make_schedule, exponential_decay\n",
    "from jax.example_libraries.stax import Dense, Gelu, serial\n",
    "from jax.nn.initializers import glorot_uniform, normal, glorot_normal\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identify the type of models to used here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spifol_archs import MLP, modified_MLP, FNOBlock2D, Permute, Dense, complex_adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the grid for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64          ## number of pixels per direction\n",
    "Lx = 1.0                     # length in x\n",
    "Ly = 1.0                     # length in y\n",
    "x = np.linspace(0, Lx, n)\n",
    "y = np.linspace(0, Ly, n)\n",
    "xx, yy = np.meshgrid(x, y) #this function used for creating the 2D array where in xx has rows are repeated on based on the no. of element of y and in yy columns are repeated on based on the no. of element of x array.\n",
    "# print(xx)\n",
    "#converting numpy array to the jax array\n",
    "xx_jax = jnp.array(xx)   \n",
    "yy_jax = jnp.array(yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "materials based on phase-constrant ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "phase_contrast = 5   ## phase contrast is the ratio of the refractive index of the material to the refractive index of the fluid\n",
    "lambda_m = 23.19/phase_contrast   ## lambda_m is the Lame's constant for the material\n",
    "mu_m =  29.51/phase_contrast     ## mu_m is the shear modulus for the material\n",
    "lambda_f = 23.19 ## Lame's constant for the fluid\n",
    "mu_f = 29.51               ## posisson's considered to be constant\n",
    "N =n  ## number of pixels per direction\n",
    "ndim = 2 ## number of dimensions ## 2D problem x and y dimesions\n",
    "\n",
    "# material tangents or stiffness tensor or elasticity tensor which is calulated using the hooke's law for isotropic material sigma = C:epsilon(formula of hooke's law)\n",
    "# initialising the material tangent and fluid tangent as 4th order tensors\n",
    "# elasticity tensor is the 4th order tensor that why we given the input for 4th times in the zeros function\n",
    "C_m = np.zeros((ndim,ndim,ndim,ndim)) ## 4th order tensor   ## C_m is the material tangent ## ndim is the number of dimensions ## 4th order tensor is the tensor of rank 4\n",
    "C_f = np.zeros((ndim,ndim,ndim,ndim)) ## 4th order tensor \n",
    "\n",
    "\n",
    "# identity matrix of rank 2\n",
    "iden = jnp.eye(ndim) ## identity tensor of rank 2 ## ndim is the number of dimensions\n",
    "\n",
    "C_m = lambda_m * jnp.einsum('ij,kl->ijkl', iden, iden) + \\\n",
    "     mu_m * (jnp.einsum('ik,jl->ijkl', iden, iden) + jnp.einsum('il,jk->ijkl', iden, iden)) ## einsum is used for the tensor contraction \n",
    "\n",
    "C_f = lambda_f * jnp.einsum('ij,kl->ijkl', iden, iden) + \\\n",
    "     mu_f * (jnp.einsum('ik,jl->ijkl', iden, iden) + jnp.einsum('il,jk->ijkl', iden, iden))\n",
    "\n",
    "# Averaging C_m and C_f to get C_0 (refrence material tangent)\n",
    "C_0 = (C_m + C_f) / 2\n",
    "\n",
    "# print(C_m)\n",
    "# print(C_f)\n",
    "# print(C_m)\n",
    "# print(C_m.shape)\n",
    "# print(C_0)\n",
    "# print(C_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = Lx /n ## delta is the grid spacing\n",
    "h = 2*jnp.pi/Lx ## h is the wave number \n",
    "k = jnp.concatenate((jnp.arange(0, n/2 + 1), jnp.arange(-n/2 + 1, 0, 1)))   ## jnp.arange(0, n/2 + 1) is the array from 0 to n/2+1 and jnp.arange(-n/2 + 1, 0, 1) is the array from -n/2+1 to 0 and jnp.concatenate is used to addition(or write in single array) two array the two arrays  ## k is the wave number\n",
    "\n",
    "#kvec = 1j*h*k ## kvec is the wave number vector\n",
    "kvec = 1j*jnp.sin(h*delta*k)/ delta ## kvec is the wave number vector\n",
    "kp, kq = jnp.meshgrid(kvec, kvec, indexing='ij')  # Note: Adjust if your indexing convention differs\n",
    "Wk2 = jnp.stack([kp, kq], axis=-1)  # Shape (n, n, 2), each [kp, kq] for all points\n",
    "mask = (kvec[:, None] == 0) & (kvec[None, :] == 0) # mask is always same in every setup\n",
    "\n",
    "# print(k)\n",
    "# print(k.shape)\n",
    "# print(kvec)\n",
    "# print(kvec.shape)\n",
    "# print(kp)\n",
    "# print(kp.shape)\n",
    "# print(Wk2)\n",
    "# print(Wk2.shape)\n",
    "# print(mask)\n",
    "# print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Lippmann-Schwinger-operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acousitc tensors\n",
    "\n",
    "def compute_A(p, q, C_0, kvec): # Note: C_0 and kvec are passed as arguments\n",
    "    Wk = jnp.array([kvec[p], kvec[q]]) # Shape (2,)\n",
    "    A = jnp.einsum('ijkl,j,l->ik', C_0, Wk, Wk).astype(jnp.complex64) # Shape (n, n)\n",
    "    return A\n",
    "def invert_A(A):\n",
    "    \n",
    "    # Invert A if the determinant is not zero; otherwise, return zeros\n",
    "    return jnp.where(det(A) != 0,inv(A), jnp.zeros_like(A))\n",
    "\n",
    "#Lippmann Schwinger operator# Define the macroscopic loading\n",
    "ep  = jnp.zeros((n, n, ndim, ndim))\n",
    "epN = jnp.zeros((n, n, ndim, ndim))\n",
    "# Prescribed macroscopic strain and load steps\n",
    "ep1 = jnp.array([[0.05, 0.00], [0.00, 0.00]])\n",
    "\n",
    "def fft2(x):\n",
    "    \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "    return fftn(x, axes=(0, 1))\n",
    "\n",
    "def ifft2(x):\n",
    "    \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "    return ifftn(x, axes=(0, 1))\n",
    "## initiliazation of strain fields\n",
    "ep = jnp.broadcast_to(ep1, ep.shape)\n",
    "ep_bar = ep\n",
    "Fep = jnp.zeros((n, n, 2, 2), dtype=complex)\n",
    "# Apply FFT across the components of ep\n",
    "Fep= fft2(ep)\n",
    "# Vectorizing compute_A over both p and q using vmap\n",
    "vmapped_compute_A = vmap(vmap(compute_A, (None, 0, None, None), out_axes=0), (0, None, None, None), out_axes=0)\n",
    "\n",
    "# Computing A for all pairs of (p, q)\n",
    "A_matrix = vmapped_compute_A(jnp.arange(n), jnp.arange(n), C_0, kvec)\n",
    "\n",
    "# Vectorizing invert_A over the resulting matrix\n",
    "vmapped_invert_A = vmap(vmap(invert_A, in_axes=0, out_axes=0), in_axes=0, out_axes=0)\n",
    "\n",
    "# Inverting A where applicable\n",
    "A_invGlobal = vmapped_invert_A(A_matrix).astype(jnp.complex64)\n",
    "\n",
    "# Vectorized computation using vmap\n",
    "def compute_green_tensor_element(pq):\n",
    "    p, q = pq\n",
    "    A_inv = A_invGlobal[p, q, :, :]\n",
    "    Wk2 = jnp.array([kvec[p], kvec[q]])\n",
    "    xi = Wk2\n",
    "    result = 0.5 * (jnp.einsum('ik,j,l->ijkl', A_inv, xi, xi) +\n",
    "                    jnp.einsum('il,k,j->ijkl', A_inv, xi, xi))\n",
    "    return result\n",
    "\n",
    "def compute_greens_tensor_vmap():\n",
    "    pq_indices = jnp.array(np.meshgrid(jnp.arange(n), jnp.arange(n), indexing='ij')).reshape(2, -1).T\n",
    "    vectorized_compute = vmap(compute_green_tensor_element, (0,), 0)\n",
    "    Greens_tensor_vmap = vectorized_compute(pq_indices).reshape(n, n, 2, 2, 2, 2)\n",
    "    return Greens_tensor_vmap\n",
    "\n",
    "#Greens_tensor_original = compute_greens_tensor_original()\n",
    "Greens_tensor = compute_greens_tensor_vmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the macroscopic loading\n",
    "ep  = jnp.zeros((n, n, ndim, ndim))\n",
    "epN = jnp.zeros((n, n, ndim, ndim))\n",
    "# Prescribed macroscopic strain and load steps\n",
    "ep1 = jnp.array([[0.05, 0.00], [0.00, 0.00]])\n",
    "\n",
    "def fft2(x):\n",
    "    \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "    return fftn(x, axes=(0, 1))\n",
    "\n",
    "def ifft2(x):\n",
    "    \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "    return ifftn(x, axes=(0, 1))\n",
    "## initiliazation of strain fields\n",
    "ep = jnp.broadcast_to(ep1, ep.shape)\n",
    "ep_bar = ep\n",
    "Fep = jnp.zeros((n, n, 2, 2), dtype=complex)\n",
    "# Apply FFT across the components of ep\n",
    "Fep= fft2(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 19)\n",
      "(100, 19)\n"
     ]
    }
   ],
   "source": [
    "# # data (microstrucutre loading) based Fourier based samples\n",
    "fpath = './data/'\n",
    "coeff_vec_samples = np.load(f\"{fpath}coeff_vec.npy\")  ## coeff_vec of size (10500,16)\n",
    "\n",
    "phi0_samples = np.load(f\"{fpath}phi0.npy\")\n",
    "\n",
    "l_samples = np.load(f\"{fpath}l_data.npy\")\n",
    "m_samples = np.load(f\"{fpath}m_data.npy\")\n",
    "\n",
    "# print(l_samples.shape, m_samples.shape)\n",
    "\n",
    "coeff_vec_train = jnp.array(coeff_vec_samples[0:8000])\n",
    "phi0_train      = jnp.array(phi0_samples[0:8000]).reshape(8000,1)\n",
    "l_train = jnp.array(l_samples[0:8000]).reshape(8000,1)\n",
    "m_train = jnp.array(m_samples[0:8000]).reshape(8000,1)\n",
    "\n",
    "data_train = jnp.concatenate([coeff_vec_train, phi0_train, l_train, m_train], axis=1)\n",
    "print(data_train.shape)\n",
    "\n",
    "\n",
    "# # Select the range 8100 to 8120\n",
    "coeff_vec_test = jnp.array(coeff_vec_samples[8050:8150])\n",
    "phi0_test = jnp.array(phi0_samples[8050:8150]).reshape(100, 1)  # Reshaped to match\n",
    "l_test = jnp.array(l_samples[8050:8150]).reshape(100, 1)\n",
    "m_test = jnp.array(m_samples[8050:8150]).reshape(100, 1)\n",
    "\n",
    "# # Concatenate to form the test data\n",
    "data_test = jnp.concatenate([coeff_vec_test, phi0_test, l_test, m_test], axis=1)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data genrator to make randomized batches\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "\n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        u = self.__data_generation(subkey)\n",
    "        return u\n",
    "\n",
    "    #@partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPiFOL for linear elasticity\n",
    "class SPiFOL:\n",
    "    def __init__(self, arch, N, layers, fno_layers, lr, activation, norm_par):\n",
    "        self.arch = arch\n",
    "        self.N = N\n",
    "        self.lr = lr\n",
    "        self.norm_par = norm_par\n",
    "\n",
    "        # Initialize the network based on architecture type\n",
    "        if arch == 'MLP':\n",
    "            self.N_init, self.N_apply = MLP(layers, activation=activation)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        elif arch == 'modified_MLP':\n",
    "            self.N_init, self.N_apply = modified_MLP(layers, activation=activation)\n",
    "            params = self.N_init(random.PRNGKey(1234))\n",
    "        elif arch == 'FNO':\n",
    "            self.N_init, self.N_apply = serial(*fno_layers)\n",
    "            _, params = self.N_init(random.PRNGKey(1234), (-1, N, N, 3))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture!\")\n",
    "\n",
    "        self.params = params\n",
    "         # Optimizer setup\n",
    "        self.opt_init, self.opt_update, self.get_params = complex_adam(\n",
    "            jax.example_libraries.optimizers.exponential_decay(\n",
    "                lr, decay_steps=2000, decay_rate=0.9\n",
    "            )\n",
    "        )\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "\n",
    "        # Logging losses\n",
    "        self.total_loss_log = []\n",
    "        self.loss_test_log = []\n",
    "        self.loss_exx_log = []\n",
    "        self.loss_eyy_log = []\n",
    "        self.loss_exy_log = []\n",
    "\n",
    "        # Initialize optimizer state\n",
    "        self.opt_state = self.opt_init(self.params)\n",
    "        _, self.unravel = ravel_pytree(params)  # Assuming all networks have the same structure\n",
    "        self.itercount = itertools.count()\n",
    "        \n",
    "        self.total_loss_log = []\n",
    "        self.loss_test_log  = []\n",
    "        self.loss_exx_log   = []\n",
    "        self.loss_eyy_log   = []\n",
    "        self.loss_exy_log   = []\n",
    "        self.sigma = []\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    \n",
    "    def full_sample(self, paramvec):   #only for Fourier based samples\n",
    "        x_freqs = jnp.array([0,1,2,3])\n",
    "        y_freqs = jnp.array([0,1,2,3])\n",
    "        coeff_vec = paramvec[0:16]\n",
    "        phi_0      = paramvec[16]\n",
    "        l         = paramvec[17]\n",
    "        m         = paramvec[18]\n",
    "        phi_str = jnp.zeros_like(xx_jax)\n",
    "        phi_str += phi_0/2.0\n",
    "        coeff_counter = 0\n",
    "        for freq_x in x_freqs:\n",
    "          for freq_y in y_freqs:\n",
    "            phi_str += coeff_vec[coeff_counter] * jnp.cos(freq_x * 2*jnp.pi * xx_jax) * jnp.cos(freq_y * 2*jnp.pi * yy_jax)\n",
    "            coeff_counter += 1\n",
    "        E =  (sigmoid(m*(phi_str-l))+0.05)/1.05\n",
    "        return E\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def material_model(self, C_f, C_m, ep, E):\n",
    "        # Compute the product for both fiber and matrix, then select based on the mask\n",
    "        # Note: einsum handles the contraction over the last two dimensions of ep with the elasticity tensors\n",
    "        s_fiber = jnp.einsum('ijkl, pqkl -> pqij', C_f, ep)\n",
    "        s_matrix = jnp.einsum('ijkl, pqkl -> pqij', C_m, ep)\n",
    "        # Combine results based on the mask\n",
    "        s = jnp.einsum('pqij,pq->pqij',s_fiber,E) + jnp.einsum('pqij,pq->pqij',s_matrix,1.0-E)\n",
    "        return s\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def material_model_C_0(self, C_0, ep):\n",
    "        # Compute the product for both fiber and matrix, then select based on the mask\n",
    "        # Note: einsum handles the contraction over the last two dimensions of ep with the elasticity tensors\n",
    "        s = jnp.einsum('ijkl, pqkl -> pqij', C_0, ep)\n",
    "        return s\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def fft2(self, x):\n",
    "        \"\"\"Applies a 2D FFT over the first two dimensions of the input array x.\"\"\"\n",
    "        return fftn(x, axes=(0, 1))\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def ifft2(self, x):\n",
    "        \"\"\"Applies a 2D inverse FFT over the first two dimensions of the input array x.\"\"\"\n",
    "        return ifftn(x, axes=(0, 1))\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def operator_net(self, params, E):\n",
    "        if self.arch in ['MLP', 'modified_MLP']:\n",
    "            # MLP or modified MLP version\n",
    "            E1 = E.flatten()\n",
    "            O = self.N_apply(params, E1)  # Directly apply the network\n",
    "            O = O.reshape(self.N, self.N, 3)  # Reshape output to match strain components\n",
    "            O = O / self.norm_par  # Normalize the output\n",
    "            out = jnp.zeros((self.N, self.N, 2, 2))  # Initialize 2x2 strain tensor\n",
    "            out = out.at[:, :, 0, 0].set(O[:, :, 0])  # Set epsilon_xx\n",
    "            out = out.at[:, :, 1, 1].set(O[:, :, 1])  # Set epsilon_yy\n",
    "            out = out.at[:, :, 0, 1].set(O[:, :, 2])  # Set epsilon_xy\n",
    "            out = out.at[:, :, 1, 0].set(O[:, :, 2])  # Set epsilon_yx (symmetry)\n",
    "\n",
    "        elif self.arch == 'FNO':\n",
    "            # FNO version\n",
    "            E1 = E.reshape(self.N, self.N)  # Reshape input strain tensor\n",
    "            input_FNO = jnp.stack([E1, self.xx_jax, self.yy_jax], axis=-1)  # Combine with spatial grid\n",
    "            input_FNO = input_FNO.reshape(-1, self.N, self.N, 3)  # Reshape for FNO\n",
    "            O = self.N_apply(params, input_FNO)  # Apply the FNO network\n",
    "            O = O.reshape(self.N, self.N, 3)  # Reshape output\n",
    "            O = O / self.norm_par  # Normalize the output\n",
    "            out = jnp.zeros((self.N, self.N, 2, 2))  # Initialize 2x2 strain tensor\n",
    "            out = out.at[:, :, 0, 0].set(O[:, :, 0])  # Set epsilon_xx\n",
    "            out = out.at[:, :, 1, 1].set(O[:, :, 1])  # Set epsilon_yy\n",
    "            out = out.at[:, :, 0, 1].set(O[:, :, 2])  # Set epsilon_xy\n",
    "            out = out.at[:, :, 1, 0].set(O[:, :, 2])  # Set epsilon_yx (symmetry)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported architecture type!\")\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_single(self, params, batch):\n",
    "        input = batch\n",
    "\n",
    "        E = self.full_sample(batch) # only is emplyoed when we call fourier based samples\n",
    "        E1 = E.reshape(n,n)\n",
    "\n",
    "        eps = self.operator_net(params, E)\n",
    "        #print(eps.shape)\n",
    "\n",
    "        Fsi = jnp.zeros((n, n, 2, 2), dtype=complex)\n",
    "        Fep = jnp.zeros((n, n, 2, 2), dtype=complex)\n",
    "        sigma  = self.material_model(C_f, C_m, eps, E1)\n",
    "        sigma0 = self.material_model_C_0((C_f+C_m)/2.00, eps)\n",
    "        Fsi      = self.fft2(sigma-sigma0)\n",
    "\n",
    "        # Calculate Fts using einsum\n",
    "        # Using einsum to compute the polarization stress\n",
    "        Fts = jnp.zeros((n, n, 2, 2), dtype=complex)\n",
    "        # Perform the einsum operation to fill in Fep initially\n",
    "        Fts = jnp.einsum('pqijkl,pqij->pqkl', Greens_tensor, Fsi)\n",
    "        ts = jnp.real(self.ifft2(Fts))\n",
    "        loss_eps1 = jnp.sum(jnp.abs((eps[:,:,0,0]+ts[:,:,0,0]-ep_bar[:,:,0,0])**2))\n",
    "        loss_eps2 = jnp.sum(jnp.abs((eps[:,:,1,1]+ts[:,:,1,1]-ep_bar[:,:,1,1])**2))\n",
    "        loss_eps3 = (jnp.sum(jnp.abs((eps[:,:,0,1]+ts[:,:,0,1]-ep_bar[:,:,0,1])**2)) +\n",
    "                     jnp.sum(jnp.abs((eps[:,:,1,0]+ts[:,:,1,0]-ep_bar[:,:,1,0])**2)))\n",
    "        return loss_eps1*1.0, loss_eps2*7.0, loss_eps3*5.0   # these factor are utilized for this set of macroscopic loading\n",
    "        # it needs to be revise or it can be combined with Neural tnagent kernels to become more precise\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_batches(self, params, batch):\n",
    "        # batch losses\n",
    "        loss_exx, loss_eyy, loss_exy = vmap(self.loss_single, (None, 0))(params, batch)\n",
    "        loss_exx  = jnp.mean(loss_exx)\n",
    "        loss_eyy  = jnp.mean(loss_eyy)\n",
    "        loss_exy  = jnp.mean(loss_exy)\n",
    "        return loss_exx, loss_eyy, loss_exy\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_T(self, params, batch):\n",
    "        # total loss calculation\n",
    "        loss_exx, loss_eyy, loss_exy = self.loss_batches(params, batch)\n",
    "        return loss_exx + loss_eyy + loss_exy\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss_T)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "    def train(self, dataset, datatest, nIter=10000):\n",
    "        data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "\n",
    "        for it in pbar:\n",
    "            batch = next(data)\n",
    "            batch = jnp.array(batch)\n",
    "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
    "\n",
    "            # Logger (log the loss every 100 iterations)\n",
    "            if it % 10 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                loss = self.loss_T(params, batch)\n",
    "                loss_exx, loss_eyy, loss_exy = self.loss_batches(params, batch)\n",
    "                loss_test = self.loss_T(params, datatest)\n",
    "                #error = self.L2error(params, batch)\n",
    "                self.total_loss_log.append(loss)\n",
    "                self.loss_test_log.append(loss_test)\n",
    "                self.loss_exx_log.append(loss_exx)\n",
    "                self.loss_eyy_log.append(loss_eyy)\n",
    "                self.loss_exy_log.append(loss_exy)\n",
    "                pbar.set_postfix({'Loss': loss, 'Relative L2 error': loss})\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def pred(self, params, E):\n",
    "        strain = self.operator_net(params, E)\n",
    "        return strain\n",
    "\n",
    "\n",
    "    def plot_losses(self, save_as):\n",
    "        # Convert loss array and jax numpy array for plotting\n",
    "        total_loss = jnp.asarray(self.total_loss_log)\n",
    "        total_loss1 = jnp.asarray(self.loss_test_log)\n",
    "        exx_loss = jnp.asarray(self.loss_exx_log)\n",
    "        eyy_loss = jnp.asarray(self.loss_eyy_log)\n",
    "        exy_loss = jnp.asarray(self.loss_exy_log)\n",
    "        #print(total_loss)\n",
    "        color = tab20.colors\n",
    "\n",
    "         #print(x_axis)\n",
    "        # Create plot\n",
    "        plt.figure(constrained_layout=True)\n",
    "        ax = plt.subplot(111)\n",
    "\n",
    "        plt.semilogy(x_axis, total_loss, label=\"Train\", c=color[0])\n",
    "        plt.semilogy(x_axis, total_loss1, label=\"Test\", c=color[6])\n",
    "        #plt.semilogy(x_axis, mm_loss, label=\"Material Model\", c=color[1])\n",
    "        #plt.semilogy(x_axis, div_loss, label=\"Div Loss\", c=color[2])\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05, 1))\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])\n",
    "        plt.savefig(save_as + \"Total_loss.png\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data trainig samples\n",
    "data_train1 = DataGenerator(data_train, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "fno_layers = [\n",
    "    Dense(32),\n",
    "    Permute(\"ijkl->iljk\"),\n",
    "    FNOBlock2D(32),\n",
    "    Gelu, # activation can be changed here\n",
    "    FNOBlock2D(32),\n",
    "    Gelu,\n",
    "    FNOBlock2D(32),\n",
    "    Permute(\"ijkl->iklj\"),\n",
    "    Dense(128),\n",
    "    Gelu,\n",
    "    Dense(3),\n",
    "]\n",
    "model_fno = SPiFOL(\n",
    "    arch='FNO',\n",
    "    N=64, # latent size\n",
    "    layers=None,\n",
    "    fno_layers=fno_layers,\n",
    "    lr=0.001,\n",
    "    activation=jax.nn.relu, # here its not used\n",
    "    norm_par=1000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [64**2, 1024, 1024, (64**2)*3] #last layer and first layer should be fixed\n",
    "model_mlp = SPiFOL(\n",
    "    arch='MLP', # or 'modified_mlp'\n",
    "    N=64,\n",
    "    layers=layers,\n",
    "    fno_layers=None,\n",
    "    lr=0.001,\n",
    "    activation=jax.nn.relu,\n",
    "    norm_par=1000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [1:02:32<00:00,  7.99it/s, Loss=0.07446858, Relative L2 error=0.07446858]  \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "epochs = 30000\n",
    "model_mlp.train(data_train1, data_test, nIter = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FNOBlock2D() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading the trained model \u001b[39;00m\n\u001b[0;32m      2\u001b[0m fno_layers \u001b[38;5;241m=\u001b[39m [Dense(\u001b[38;5;241m32\u001b[39m), \n\u001b[0;32m      3\u001b[0m           Permute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mijkl->iljk\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m----> 4\u001b[0m           \u001b[43mFNOBlock2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m, Gelu,\n\u001b[0;32m      5\u001b[0m           FNOBlock2D(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m), Gelu,\n\u001b[0;32m      6\u001b[0m           FNOBlock2D(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[0;32m      7\u001b[0m           Permute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mijkl->iklj\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m           Dense(\u001b[38;5;241m128\u001b[39m), \n\u001b[0;32m      9\u001b[0m           Gelu,\n\u001b[0;32m     10\u001b[0m           Dense(\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m     11\u001b[0m model_fno \u001b[38;5;241m=\u001b[39m SPiFOL(\n\u001b[0;32m     12\u001b[0m     arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNO\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \u001b[38;5;66;03m# latent size\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     norm_par\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m     19\u001b[0m ) \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# trianed model params are above\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: FNOBlock2D() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# loading the trained model \n",
    "fno_layers = [Dense(32), \n",
    "          Permute(\"ijkl->iljk\"), \n",
    "          FNOBlock2D(32, 16), Gelu,\n",
    "          FNOBlock2D(32, 16), Gelu,\n",
    "          FNOBlock2D(32, 16),\n",
    "          Permute(\"ijkl->iklj\"),\n",
    "          Dense(128), \n",
    "          Gelu,\n",
    "          Dense(3)]\n",
    "model_fno = SPiFOL(\n",
    "    arch='FNO',\n",
    "    N=64, # latent size\n",
    "    layers=None,\n",
    "    fno_layers=fno_layers,\n",
    "    lr=0.001,\n",
    "    activation=jax.nn.relu, # here its not used\n",
    "    norm_par=1000.0\n",
    ") \n",
    "# trianed model params are above\n",
    "flat_params  = np.load(\"./data/FNO_32_16_multiple.npy\")\n",
    "params1 = model_fno.unravel(flat_params)\n",
    "model_fno.params = params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m model_fno\u001b[38;5;241m.\u001b[39mfull_sample(data)\n\u001b[0;32m      4\u001b[0m E1 \u001b[38;5;241m=\u001b[39m (data)\u001b[38;5;241m.\u001b[39mreshape(n,n)\n\u001b[1;32m----> 5\u001b[0m eps \u001b[38;5;241m=\u001b[39m model_fno\u001b[38;5;241m.\u001b[39moperator_net(\u001b[43mparams1\u001b[49m, E1)\n\u001b[0;32m      6\u001b[0m sigma  \u001b[38;5;241m=\u001b[39m model_fno\u001b[38;5;241m.\u001b[39mmaterial_model(C_f, C_m, eps, E1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'params1' is not defined"
     ]
    }
   ],
   "source": [
    "# prediction \n",
    "data = data_test[2]\n",
    "data = model_fno.full_sample(data)\n",
    "E1 = (data).reshape(n,n)\n",
    "eps = model_fno.operator_net(params1, E1)\n",
    "sigma  = model_fno.material_model(C_f, C_m, eps, E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
